{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7317a458",
   "metadata": {},
   "source": [
    "### Notebook to demonstrate Image Classification workflow\n",
    "\n",
    "Transfer learning is the process of transferring learned features from one application to another. It is a commonly used training technique where you use a model trained on one task and re-train to use it on a different task. Train Adapt Optimize (TAO) Toolkit  is a simple and easy-to-use Python based AI toolkit for taking purpose-built AI models and customizing them with users' own data.\n",
    "\n",
    "![image](https://d29g4g2dyqv443.cloudfront.net/sites/default/files/akamai/TAO/tlt-tao-toolkit-bring-your-own-model-diagram.png)\n",
    "\n",
    "### Sample prediction for an Image Classification model\n",
    "<img align=\"center\" src=\"../example_images/sample_image_classification.jpg\">\n",
    "\n",
    "### The workflow in a nutshell\n",
    "\n",
    "- Creating a dataset\n",
    "- Upload dataset to the service\n",
    "- Getting a PTM from NGC\n",
    "- Model Actions\n",
    "    - Train (Normal/AutoML)\n",
    "    - Evaluate\n",
    "    - Prune, retrain\n",
    "    - Export\n",
    "    - TAO-Deploy\n",
    "    - Inference on TAO\n",
    "\n",
    "### Table of contents\n",
    "\n",
    "1. [Create datasets ](#head-1)\n",
    "1. [List the created datasets](#head-2)\n",
    "1. [Create an experiment](#head-4)\n",
    "1. [List experiments](#head-5)\n",
    "1. [Assign train, eval datasets](#head-6)\n",
    "1. [Assign PTM](#head-7)\n",
    "1. [View hyperparameters that are enabled by default](#head-8)\n",
    "1. [Set AutoML related configurations](#head-9)\n",
    "1. [Actions](#head-10)\n",
    "1. [Train](#head-11)\n",
    "1. [Evaluate](#head-12)\n",
    "1. [Optimize: Apply specs for prune](#head-14)\n",
    "1. [Optimize: Apply specs for retrain](#head-15)\n",
    "1. [Optimize: Run actions](#head-16)\n",
    "1. [Export](#head-17)\n",
    "1. [TRT Engine generation using TAO-Deploy](#head-19)\n",
    "1. [TAO inference](#head-20)\n",
    "\n",
    "### Requirements\n",
    "Please find the server requirements [here](https://docs.nvidia.com/tao/tao-toolkit/text/tao_toolkit_api/api_setup.html#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "84e19481",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import requests\n",
    "import uuid\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "import subprocess\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba98734",
   "metadata": {},
   "source": [
    "### FIXME\n",
    "\n",
    "1. Assign a model_name in FIXME 1\n",
    "1. Assign a workdir in FIXME 2\n",
    "1. Assign the ip_address and port_number in FIXME 3 ([info](https://docs.nvidia.com/tao/tao-toolkit/text/tao_toolkit_api/api_rest_api.html))\n",
    "1. Assign the ngc_api_key variable in FIXME 4\n",
    "1. (Optional) Enable AutoML if needed in FIXME 5\n",
    "1. (Optional) Choose between bayesian and hyperband automl_algorithm in FIXME 6 (If automl was enabled in FIXME5)\n",
    "1. Choose to download jobs or not in FIXME 7\n",
    "1. Choose between default and custom dataset in FIXME 8\n",
    "1. Assign path of DATA_DIR in FIXME 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "c3eefd59",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define model_name workspaces and other variables\n",
    "# Available models (#FIXME 1):\n",
    "# 1. classification_pyt - https://docs.nvidia.com/tao/tao-toolkit/text/image_classification.html\n",
    "# 2. classification_tf1 - https://docs.nvidia.com/tao/tao-toolkit/text/image_classification.html\n",
    "# 3. classification_tf2 - https://docs.nvidia.com/tao/tao-toolkit/text/image_classification_tf2.html\n",
    "# 4. multitask_classification - https://docs.nvidia.com/tao/tao-toolkit/text/multitask_image_classification.html\n",
    "# classification is the same as multi-class classification\n",
    "\n",
    "model_name = \"classification_tf2\" # FIXME1 (Add the model name from the above mentioned list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "0ef12c5f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "workdir = \"/home/zyw/tao-env/siemens_product_cla\" # FIXME2\n",
    "host_url = \"http://192.168.1.85:31951\" # FIXME3 example: https://10.137.149.22:32334\n",
    "# In host machine, node ip_address and port number can be obtained as follows,\n",
    "# ip_address: hostname -i\n",
    "# port_number: kubectl get service ingress-nginx-controller -o jsonpath='{.spec.ports[0].nodePort}'\n",
    "ngc_api_key = \"YTVmYWs3aDgxZ2Q2aG5oY3Yyc2RwZG9na2Q6MDFjMmUxMjMtZDNhZC00MWJlLWFmZGMtMGU3ZTc1OThjMGY3\" # FIXME4 example: (Add NGC API key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "ce5261d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "automl_enabled = True # FIXME5 set to True if you want to run automl for the model chosen in the previous cell\n",
    "automl_algorithm = \"bayesian\" # FIXME6 example: bayesian/hyperband\n",
    "# FIXME7 Defaulted to False as downloading jobs from service to your machine takes time\n",
    "# Set to True if you want to download jobs where examples have been provided like for train, export, inference.\n",
    "download_jobs = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "15a9b160",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User ID a1c02cba-b62b-52f9-9e49-e3de0e5b66ab\n",
      "JWT eyJraWQiOiJFUkNPOklCWFY6TjY2SDpOUEgyOjNMRlQ6SENVVToyRkFTOkJJTkw6WkxKRDpNWk9ZOkRVN0o6TVlVWSIsImFsZyI6IlJTMjU2In0.eyJzdWIiOiJhNWZhazdoODFnZDZobmhjdjJzZHBkb2drZCIsImF1ZCI6Im5nYyIsImFjY2VzcyI6W10sImlzcyI6ImF1dGhuLm52aWRpYS5jb20iLCJvcHRpb25zIjpbXSwiZXhwIjoxNzI0MjE4OTQzLCJpYXQiOjE3MjQyMTgzNDMsImp0aSI6IjlhZmExZjQzLTk1NzYtNDAxNS05YWQ5LWU1MDJmOTQ4NmI5MCJ9.1lIV9LlaYYQzOceG_fsxpUTkuvHMvDfYwkjkgwkRVPKj0dXWyty3OG2UzsyHJf7SGz4UQLrzPWz8nL2SpPt34uOy4InczgAHchERY-PLY7gLcB-U3G36aDnALwwMwS_iVuGSztfoomKQFM-Qitn0bqEt1I4bo5DftA9Yzq8CsX-XaAMdZQe2veLwV8Wz0M7T3TCO4p7SZoysz03BW7M9ymMZMUf3d5cMJiv71FywXLbpMqdLQvdj43A50qGpjlkTEWszXA5rrAeWp-N8wbqhAmS-h9LMZ6zmXhnlRCqU5OpX1V1fpK2qja94p8NToTJ4DuebfXx25PZ-Ucraw8JXzfuX4sr1KsbjdICbrh5KOrFL2gXNbipzk13SbuivYtvoDUyG4_bZlF4J3DlQThXI3BziPPel38vJAOSPzfNJWoNZaAmKdqMYlws06_X3LVzELDHH2Z30UgkDx6LSVrZpvQ6ythXHiUgv7lCWJUTnvK8YavCO4k1TvKuJJsUcyz1LI3B6XVVjnXRZ0LdhtjXgJgwE-uSfJkS3JEyeBANSwVq030tyrWZtnpMToWe0xkczHwKf4ALE1iKUHeOa0T-F0adSIvgozuu-AbAbU4V_qDqplat4mO0Wh6dHaOAGVIUnu4H7G23ifNkUJEtogIslM92gRDWWoXkGacn3jB7N3QU\n",
      "API Calls will be forwarded to http://192.168.1.85:31951/api/v1/users/a1c02cba-b62b-52f9-9e49-e3de0e5b66ab\n"
     ]
    }
   ],
   "source": [
    "# Exchange NGC_API_KEY for JWT\n",
    "data = json.dumps({\"ngc_api_key\": ngc_api_key})\n",
    "response = requests.post(f\"{host_url}/api/v1/login\", data=data)\n",
    "assert response.status_code in (200, 201)\n",
    "assert \"user_id\" in response.json().keys()\n",
    "user_id = response.json()[\"user_id\"]\n",
    "print(\"User ID\",user_id)\n",
    "assert \"token\" in response.json().keys()\n",
    "token = response.json()[\"token\"]\n",
    "print(\"JWT\",token)\n",
    "\n",
    "# Set base URL\n",
    "base_url = f\"{host_url}/api/v1/users/{user_id}\"\n",
    "print(\"API Calls will be forwarded to\",base_url)\n",
    "\n",
    "headers = {\"Authorization\": f\"Bearer {token}\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "55ceb355",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Creating workdir\n",
    "if not os.path.isdir(workdir):\n",
    "    os.makedirs(workdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f0d2fb",
   "metadata": {},
   "source": [
    "### Function to split tar files <a class=\"anchor\" id=\"head-1.1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "05ba9421",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tarfile\n",
    "\n",
    "def split_tar_file(input_tar_path, output_dir, max_split_size=0.2*1024*1024*1024):\n",
    "\tos.makedirs(output_dir, exist_ok=True)\n",
    "\t\n",
    "\twith tarfile.open(input_tar_path, 'r') as original_tar:\n",
    "\t\tmembers = original_tar.getmembers()\n",
    "\t\tcurrent_split_size = 0\n",
    "\t\tcurrent_split_number = 0\n",
    "\t\tcurrent_split_name = os.path.join(output_dir, f'smaller_file_{current_split_number}.tar')\n",
    "\t\t\n",
    "\t\twith tarfile.open(current_split_name, 'w') as split_tar:\n",
    "\t\t\tfor member in members:\n",
    "\t\t\t\tif current_split_size + member.size <= max_split_size:\n",
    "\t\t\t\t\tsplit_tar.addfile(member, original_tar.extractfile(member))\n",
    "\t\t\t\t\tcurrent_split_size += member.size\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tsplit_tar.close()\n",
    "\t\t\t\t\tcurrent_split_number += 1\n",
    "\t\t\t\t\tcurrent_split_name = os.path.join(output_dir, f'smaller_file_{current_split_number}.tar')\n",
    "\t\t\t\t\tcurrent_split_size = 0\n",
    "\t\t\t\t\tsplit_tar = tarfile.open(current_split_name, 'w')  # Open a new split tar archive\n",
    "\t\t\t\t\tsplit_tar.addfile(member, original_tar.extractfile(member))\n",
    "\t\t\t\t\tcurrent_split_size += member.size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c5dbbd",
   "metadata": {},
   "source": [
    "### Set dataset type, format <a class=\"anchor\" id=\"head-1.1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743950c8",
   "metadata": {},
   "source": [
    "**For multi-class classification:**\n",
    "\n",
    "We will be using the pascal `VOC dataset` for the tutorial. To find more details please visit [here](http://host.robots.ox.ac.uk/pascal/VOC/voc2012/index.html#devkit). Please download the [dataset](http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar) to the environment variable `$DATA_DIR`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b13071b",
   "metadata": {},
   "source": [
    "**If using custom dataset; it should follow this dataset structure, and skip running** \"**Split dataset into train and val sets**\"\n",
    "```\n",
    "DATA_DIR\n",
    "├── classes.txt\n",
    "├── images_test\n",
    "│   ├── class_name_1\n",
    "│   │   ├── image_name_1.jpg\n",
    "│   │   ├── image_name_2.jpg\n",
    "│   │   ├── ...\n",
    "|   |   ... \n",
    "│   └── class_name_n\n",
    "│       ├── image_name_3.jpg\n",
    "│       ├── image_name_4.jpg\n",
    "│       ├── ...\n",
    "├── images_train\n",
    "│   ├── class_name_1\n",
    "│   │   ├── image_name_5.jpg\n",
    "│   │   ├── image_name_6.jpg\n",
    "|   |   ...\n",
    "│   └── class_name_n\n",
    "│       ├── image_name_7.jpg\n",
    "│       ├── image_name_8.jpg\n",
    "│       ├── ...\n",
    "|\n",
    "└── images_val\n",
    "    ├── class_name_1\n",
    "    │   ├── image_name_9.jpg\n",
    "    │   ├── image_name_10.jpg\n",
    "    │   ├── ...\n",
    "    |   ...\n",
    "    └── class_name_n\n",
    "        ├── image_name_11.jpg\n",
    "        ├── image_name_12.jpg\n",
    "        ├── ...\n",
    "```\n",
    "- Each class name folder should contain the images corresponding to that class\n",
    "- Same class name folders should be present across images_test, images_train and images_val\n",
    "- classes.txt is a file which contains the names of all classes (each name in a separate line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e026063",
   "metadata": {},
   "source": [
    "**For multi-task classification:**\n",
    "\n",
    "We will be using the `Fashion Product Images (Small)` for the tutorial. This dataset is available on Kaggle.In this tutorial, our trained classification network will perform three tasks: article category classification, base color classification and target season classification.\n",
    "\n",
    "To download the dataset, you will need a Kaggle account. After login, you can download the dataset zip file [here](https://www.kaggle.com/paramaggarwal/fashion-product-images-small). The downloaded file is archive.zip with a subfolder called myntradataset. Unzip contents in this subfolder to your workdir created in the cell above and you should have a folder called images and a CSV file called styles.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9c5563",
   "metadata": {},
   "source": [
    "**If using custom dataset; it should follow this dataset structure**\n",
    "```\n",
    "DATA_DIR\n",
    "├── images\n",
    "│   ├── image_name_1.jpg\n",
    "│   ├── image_name_2.jpg\n",
    "|   |   ├── ...\n",
    "├── styles.csv\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "1163dcab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "default\n"
     ]
    }
   ],
   "source": [
    "# Create train dataset\n",
    "ds_type = \"image_classification\"\n",
    "if model_name == \"classification_pyt\":\n",
    "    ds_format = model_name\n",
    "elif \"classification_\" in model_name:\n",
    "    ds_format = \"default\"\n",
    "elif model_name == \"multitask_classification\":\n",
    "    ds_format = \"custom\"\n",
    "print(ds_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "74b91e04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'classification_tf2'"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "1d1af2fd",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_to_be_used = \"custom\" #FIXME8 #default/custom; default for the dataset used in this tutorial notebook; custom for a different dataset\n",
    "DATA_DIR = os.path.join(workdir, model_name, \"source_data\") # FIXME9\n",
    "os.environ['DATA_DIR']= DATA_DIR\n",
    "!mkdir -p $DATA_DIR\n",
    "job_map = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "e8cf64a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/zyw/tao-env/siemens_product_cla/classification_tf2/source_data'"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.abspath(DATA_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfccf137",
   "metadata": {},
   "source": [
    "### Dataset download and pre-processing <a class=\"anchor\" id=\"head-1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "338ce96b-66c7-44e6-8f42-1584b8c4d7d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/zyw/tao-env/siemens_product_cla/classification_tf2/source_data\n"
     ]
    }
   ],
   "source": [
    "print(DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "866e260b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if dataset_to_be_used == \"default\":\n",
    "#     if \"classification_\" in model_name:\n",
    "#         assert os.path.exists(os.path.join(DATA_DIR,\"VOCtrainval_11-May-2012.tar\"))\n",
    "#         !tar -xf $DATA_DIR/VOCtrainval_11-May-2012.tar -C $DATA_DIR\n",
    "#         assert (os.path.exists(f\"{DATA_DIR}/VOCdevkit/\"))\n",
    "#         !rm -rf $DATA_DIR/split\n",
    "#     elif model_name == \"multitask_classification\":\n",
    "#         assert os.path.exists(os.path.join(DATA_DIR,\"archive.zip\"))\n",
    "#         !unzip -uq $DATA_DIR/archive.zip -d $DATA_DIR/\n",
    "#         assert (os.path.exists(f\"{DATA_DIR}/images\"))\n",
    "#         assert (os.path.exists(f\"{DATA_DIR}/styles.csv\"))\n",
    "#         # Create subdirectories and remove existing files in them\n",
    "#         !mkdir -p $DATA_DIR/images_train && rm -rf $DATA_DIR/images_train/*\n",
    "#         !mkdir -p $DATA_DIR/images_val && rm -rf $DATA_DIR/images_val/*\n",
    "#         !mkdir -p $DATA_DIR/images_test && rm -rf $DATA_DIR/images_test/*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58336011",
   "metadata": {},
   "source": [
    "#### Split dataset into train and val sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "f0ed592f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'classification_tf2'"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "5d3bb5c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'custom'"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_to_be_used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "60fa8b0c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Split dataset into train and val sets\n",
    "# !python3 -m pip install numpy pandas==1.5.1 tqdm\n",
    "# if \"classification_\" in model_name and dataset_to_be_used == \"default\":\n",
    "#     !python3 ../dataset_prepare/classification/dataset_split.py\n",
    "#     assert (os.path.exists(f\"{DATA_DIR}/split/images_train/\"))\n",
    "#     assert (os.path.exists(f\"{DATA_DIR}/split/images_val/\"))\n",
    "#     assert (os.path.exists(f\"{DATA_DIR}/split/images_test/\"))\n",
    "# elif model_name == \"multitask_classification\" and dataset_to_be_used == \"default\":\n",
    "#     !python3 ../dataset_prepare/multitask_classification/dataset_split.py --max_images 10000\n",
    "#     assert (os.path.exists(f\"{DATA_DIR}/images_train/\"))\n",
    "#     assert (os.path.exists(f\"{DATA_DIR}/images_val/\"))\n",
    "#     assert (os.path.exists(f\"{DATA_DIR}/images_test/\"))\n",
    "#     assert (os.path.exists(f\"{DATA_DIR}/train.csv\"))\n",
    "#     assert (os.path.exists(f\"{DATA_DIR}/val.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc5995a",
   "metadata": {},
   "source": [
    "### Split my custom Siemens dataset into train, val and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "1b744c9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path: /home/zyw/tao-env/siemens_product_cla/classification_tf2/split/images_train\n",
      "Path: /home/zyw/tao-env/siemens_product_cla/classification_tf2/split/images_val\n",
      "Path: /home/zyw/tao-env/siemens_product_cla/classification_tf2/split/images_test\n",
      "Class names: ['ET200ecoPN', 'ET200sp', 'S7_1200', 'ET200AL', 'S7_1500']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "DATA_DIR\n",
    "# Set up directories\n",
    "DATA_DIR = os.environ.get('DATA_DIR')\n",
    "SOURCE_DIR = DATA_DIR\n",
    "IMAGES_TRAIN_DIR = os.path.join(os.path.dirname(DATA_DIR), 'split', 'images_train')\n",
    "IMAGES_VAL_DIR = os.path.join(os.path.dirname(DATA_DIR), 'split', 'images_val')\n",
    "IMAGES_TEST_DIR = os.path.join(os.path.dirname(DATA_DIR), 'split', 'images_test')\n",
    "\n",
    "# Get class names from the directory structure\n",
    "class_names = [d for d in os.listdir(SOURCE_DIR) if os.path.isdir(os.path.join(SOURCE_DIR, d))]\n",
    "# Create directories if they don't exist\n",
    "for dir_path in [IMAGES_TRAIN_DIR, IMAGES_VAL_DIR, IMAGES_TEST_DIR]:\n",
    "    print(\"Path:\", dir_path)\n",
    "    if not os.path.exists(dir_path):\n",
    "        os.makedirs(dir_path)\n",
    "        \n",
    "# Create classes.txt file\n",
    "with open(os.path.join(os.path.dirname(IMAGES_TRAIN_DIR), 'classes.txt'), 'w') as f:\n",
    "    for class_name in class_names:\n",
    "        f.write(f\"{class_name}\\n\")\n",
    "\n",
    "print(\"Class names:\", class_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "ed5a8419",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "1077d122",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original Source Dataset Distribution:\n",
      "Class           Count      Percentage\n",
      "-----------------------------------\n",
      "ET200ecoPN      226        22.07%\n",
      "ET200sp         193        18.85%\n",
      "S7_1200         189        18.46%\n",
      "ET200AL         253        24.71%\n",
      "S7_1500         163        15.92%\n",
      "Total           1024       100.00%\n"
     ]
    }
   ],
   "source": [
    "def get_source_distribution(src_dir):\n",
    "    class_distribution = {}\n",
    "    total_images = 0\n",
    "    for class_name in class_names:\n",
    "        assert os.path.exists(os.path.join(src_dir, class_name))\n",
    "        images = glob.glob(os.path.join(src_dir, class_name, '*.*'))\n",
    "        class_distribution[class_name] = len(images)\n",
    "        total_images += len(images)\n",
    "    return class_distribution, total_images\n",
    "\n",
    "# Get and print source distribution\n",
    "source_distribution, source_total = get_source_distribution(SOURCE_DIR)\n",
    "print(\"\\nOriginal Source Dataset Distribution:\")\n",
    "print(f\"{'Class':<15} {'Count':<10} {'Percentage':<10}\")\n",
    "print(\"-\" * 35)\n",
    "for class_name, count in source_distribution.items():\n",
    "    percentage = count / source_total * 100\n",
    "    print(f\"{class_name:<15} {count:<10} {percentage:.2f}%\")\n",
    "print(f\"{'Total':<15} {source_total:<10} 100.00%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "4a03984b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Splitting dataset: 100%|███████████████████████████████████████████████| 5/5 [00:00<00:00, 21.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Class Distribution:\n",
      "Class           Train                Val                  Test                 Total     \n",
      "-------------------------------------------------------------------------------------\n",
      "ET200ecoPN      171        (75.7%) 30         (13.3%) 25         (11.1%) 226       \n",
      "ET200sp         134        (69.4%) 26         (13.5%) 33         (17.1%) 193       \n",
      "S7_1200         130        (68.8%) 29         (15.3%) 30         (15.9%) 189       \n",
      "ET200AL         173        (68.4%) 47         (18.6%) 33         (13.0%) 253       \n",
      "S7_1500         109        (66.9%) 31         (19.0%) 23         (14.1%) 163       \n",
      "\n",
      "Total images: Train: 717 (70.0%), Validation: 163 (15.9%), Test: 144 (14.1%)\n",
      "\n",
      "Dataset preparation completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Function to split and copy images\n",
    "def split_and_copy(src_dir, train_dir, val_dir, test_dir, train_ratio=0.7, val_ratio=0.15):\n",
    "    class_distribution = defaultdict(lambda: defaultdict(int))\n",
    "    total_images = 0\n",
    "\n",
    "    for class_name in class_names:\n",
    "        images = glob.glob(os.path.join(src_dir, class_name, '*.*'))\n",
    "        total_images += len(images)\n",
    "        for dir_path in [train_dir, val_dir, test_dir]:\n",
    "            os.makedirs(os.path.join(dir_path, class_name), exist_ok=True)\n",
    "\n",
    "    for class_name in tqdm(class_names, desc=\"Splitting dataset\"):\n",
    "        images = glob.glob(os.path.join(src_dir, class_name, '*.*'))\n",
    "        random.shuffle(images)\n",
    "        \n",
    "        for img in images:\n",
    "            rand_val = random.random()\n",
    "            if rand_val < train_ratio:\n",
    "                dest_dir = train_dir\n",
    "                split_name = 'train'\n",
    "            elif rand_val < train_ratio + val_ratio:\n",
    "                dest_dir = val_dir\n",
    "                split_name = 'val'\n",
    "            else:\n",
    "                dest_dir = test_dir\n",
    "                split_name = 'test'\n",
    "            \n",
    "            shutil.copy2(img, os.path.join(dest_dir, class_name))\n",
    "            class_distribution[class_name][split_name] += 1\n",
    "\n",
    "    return class_distribution, total_images\n",
    "\n",
    "# Split the dataset\n",
    "class_distribution, total_images = split_and_copy(SOURCE_DIR, IMAGES_TRAIN_DIR, IMAGES_VAL_DIR, IMAGES_TEST_DIR)\n",
    "\n",
    "print(\"\\nClass Distribution:\")\n",
    "print(f\"{'Class':<15} {'Train':<20} {'Val':<20} {'Test':<20} {'Total':<10}\")\n",
    "print(\"-\" * 85)\n",
    "\n",
    "empty_classes = []\n",
    "\n",
    "for class_name in class_names:\n",
    "    train_count = class_distribution[class_name]['train']\n",
    "    val_count = class_distribution[class_name]['val']\n",
    "    test_count = class_distribution[class_name]['test']\n",
    "    total_count = train_count + val_count + test_count\n",
    "    \n",
    "    if total_count == 0:\n",
    "        empty_classes.append(class_name)\n",
    "        print(f\"{class_name:<15} No images found\")\n",
    "    else:\n",
    "        print(f\"{class_name:<15} \"\n",
    "              f\"{train_count:<10} ({train_count/total_count:.1%}) \"\n",
    "              f\"{val_count:<10} ({val_count/total_count:.1%}) \"\n",
    "              f\"{test_count:<10} ({test_count/total_count:.1%}) \"\n",
    "              f\"{total_count:<10}\")\n",
    "\n",
    "# Count images in each split\n",
    "train_count = sum(class_distribution[c]['train'] for c in class_names)\n",
    "val_count = sum(class_distribution[c]['val'] for c in class_names)\n",
    "test_count = sum(class_distribution[c]['test'] for c in class_names)\n",
    "\n",
    "print(f\"\\nTotal images: Train: {train_count} ({train_count/total_images:.1%}), \"\n",
    "      f\"Validation: {val_count} ({val_count/total_images:.1%}), \"\n",
    "      f\"Test: {test_count} ({test_count/total_images:.1%})\")\n",
    "\n",
    "if empty_classes:\n",
    "    print(\"\\nWarning: The following classes have no images:\")\n",
    "    for class_name in empty_classes:\n",
    "        print(f\"- {class_name}\")\n",
    "\n",
    "print('\\nDataset preparation completed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ac6cf3",
   "metadata": {},
   "source": [
    "### Create Tar files to upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "ec6bc3c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = os.path.join(os.path.dirname(DATA_DIR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "7e49b7e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/zyw/tao-env/siemens_product_cla/classification_tf2'"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "041ba3a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# the zipped file should be in a folder called split_tar in DATA_DIR, if not existing, create it\n",
    "split_tar_dir = os.path.join(DATA_DIR, 'split_tar')\n",
    "os.makedirs(split_tar_dir, exist_ok=True)\n",
    "\n",
    "if \"classification_\" in model_name:\n",
    "    !tar -C $DATA_DIR/split/ -czf $split_tar_dir/classification_train.tar.gz images_train classes.txt\n",
    "    !tar -C $DATA_DIR/split/ -czf $split_tar_dir/classification_val.tar.gz images_val classes.txt\n",
    "    !tar -C $DATA_DIR/split/ -czf $split_tar_dir/classification_test.tar.gz images_test classes.txt\n",
    "elif model_name == \"multitask_classification\":\n",
    "    !tar -C $DATA_DIR/ -czf $split_tar_dir/mt_classification_train.tar.gz images_train train.csv val.csv\n",
    "    !tar -C $DATA_DIR/ -czf $split_tar_dir/mt_classification_val.tar.gz images_val val.csv\n",
    "    !tar -C $DATA_DIR/ -czf $split_tar_dir/mt_classification_test.tar.gz images_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "39329c7c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "if \"classification_\" in model_name:\n",
    "    train_dataset_path =  os.path.join(split_tar_dir, \"classification_train.tar.gz\")\n",
    "    eval_dataset_path = os.path.join(split_tar_dir, \"classification_val.tar.gz\")\n",
    "    test_dataset_path = os.path.join(split_tar_dir, \"classification_test.tar.gz\")\n",
    "elif model_name == \"multitask_classification\":\n",
    "    train_dataset_path =  os.path.join(split_tar_dir, \"mt_classification_train.tar.gz\")\n",
    "    eval_dataset_path = os.path.join(split_tar_dir, \"mt_classification_val.tar.gz\")\n",
    "    test_dataset_path = os.path.join(split_tar_dir, \"mt_classification_test.tar.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "bc54270a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/zyw/tao-env/siemens_product_cla/classification_tf2/split_tar/classification_train.tar.gz'"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.abspath(train_dataset_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d85a11",
   "metadata": {},
   "source": [
    "### Create and upload train dataset <a class=\"anchor\" id=\"head-1.2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "d8f4fb4d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [201]>\n",
      "{'actions': [], 'client_url': None, 'created_on': '2024-08-21T05:34:56.135928', 'description': 'My TAO Dataset', 'docker_env_vars': {}, 'format': 'default', 'id': '1c63db8b-5281-4aea-95dd-8f7b029039dc', 'jobs': [], 'last_modified': '2024-08-21T05:34:56.135944', 'logo': 'https://www.nvidia.com', 'name': 'My Dataset', 'pull': None, 'status': 'not_present', 'type': 'image_classification', 'version': '1.0.0'}\n"
     ]
    }
   ],
   "source": [
    "data = json.dumps({\"type\":ds_type,\"format\":ds_format})\n",
    "\n",
    "endpoint = f\"{base_url}/datasets\"\n",
    "\n",
    "response = requests.post(endpoint,data=data,headers=headers)\n",
    "assert response.status_code in (200, 201)\n",
    "\n",
    "\n",
    "print(response)\n",
    "print(response.json())\n",
    "assert \"id\" in response.json().keys()\n",
    "train_dataset_id = response.json()[\"id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "5f11f0c5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n",
      "{'actions': [], 'client_url': None, 'created_on': '2024-08-21T05:34:56.135928', 'description': 'My train dataset', 'docker_env_vars': {}, 'format': 'default', 'id': '1c63db8b-5281-4aea-95dd-8f7b029039dc', 'jobs': [], 'last_modified': '2024-08-21T05:35:28.731986', 'logo': 'https://www.nvidia.com', 'name': 'Siemens Train Dataset', 'pull': None, 'status': 'not_present', 'type': 'image_classification', 'version': '1.0.0'}\n"
     ]
    }
   ],
   "source": [
    "# Update\n",
    "dataset_information = {\"name\":\"Siemens Train Dataset\",\n",
    "                       \"description\":\"My train dataset\"}\n",
    "data = json.dumps(dataset_information)\n",
    "\n",
    "endpoint = f\"{base_url}/datasets/{train_dataset_id}\"\n",
    "\n",
    "response = requests.patch(endpoint, data=data, headers=headers)\n",
    "assert response.status_code in (200, 201)\n",
    "\n",
    "\n",
    "print(response)\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "0ae7d2fc",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading 1/1 tar split\n",
      "<Response [201]>\n",
      "{'message': 'Server recieved file and upload process started'}\n"
     ]
    }
   ],
   "source": [
    "# Upload\n",
    "output_dir = os.path.join(os.path.dirname(os.path.abspath(train_dataset_path)), model_name, \"train\")\n",
    "split_tar_file(train_dataset_path, output_dir)\n",
    "for idx, tar_dataset_path in enumerate(os.listdir(output_dir)):\n",
    "    print(f\"Uploading {idx+1}/{len(os.listdir(output_dir))} tar split\")\n",
    "    files = [(\"file\",open(os.path.join(output_dir, tar_dataset_path),\"rb\"))]\n",
    "\n",
    "    endpoint = f\"{base_url}/datasets/{train_dataset_id}:upload\"\n",
    "\n",
    "    response = requests.post(endpoint, files=files, headers=headers)\n",
    "    assert response.status_code in (200, 201)\n",
    "    assert \"message\" in response.json().keys() and response.json()[\"message\"] == \"Server recieved file and upload process started\"\n",
    "\n",
    "    print(response)\n",
    "    print(response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "09988abe-1054-4c3c-8f9c-89207ff29230",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/zyw/tao-env/siemens_product_cla/classification_tf2/split_tar/classification_tf2/train\n"
     ]
    }
   ],
   "source": [
    "print(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49553a5",
   "metadata": {},
   "source": [
    "### Create and upload val dataset <a class=\"anchor\" id=\"head-1.3\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "5b760d50",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [201]>\n",
      "{'actions': [], 'client_url': None, 'created_on': '2024-08-21T05:35:46.334652', 'description': 'My TAO Dataset', 'docker_env_vars': {}, 'format': 'default', 'id': 'd79beccd-32d3-4c7b-a936-4574f52ec94b', 'jobs': [], 'last_modified': '2024-08-21T05:35:46.334666', 'logo': 'https://www.nvidia.com', 'name': 'My Dataset', 'pull': None, 'status': 'not_present', 'type': 'image_classification', 'version': '1.0.0'}\n"
     ]
    }
   ],
   "source": [
    "# Create eval dataset\n",
    "ds_type = \"image_classification\"\n",
    "if model_name == \"classification_pyt\":\n",
    "    ds_format = model_name\n",
    "else:\n",
    "    ds_format = \"default\"\n",
    "data = json.dumps({\"type\":ds_type,\"format\":ds_format})\n",
    "\n",
    "endpoint = f\"{base_url}/datasets\"\n",
    "\n",
    "response = requests.post(endpoint,data=data,headers=headers)\n",
    "assert response.status_code in (200, 201)\n",
    "\n",
    "print(response)\n",
    "print(response.json())\n",
    "assert \"id\" in response.json().keys()\n",
    "eval_dataset_id = response.json()[\"id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "1a28e2e0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n",
      "{'actions': [], 'client_url': None, 'created_on': '2024-08-21T05:35:46.334652', 'description': 'S eval dataset', 'docker_env_vars': {}, 'format': 'default', 'id': 'd79beccd-32d3-4c7b-a936-4574f52ec94b', 'jobs': [], 'last_modified': '2024-08-21T05:35:48.708111', 'logo': 'https://www.nvidia.com', 'name': 'Siemens Eval dataset', 'pull': None, 'status': 'not_present', 'type': 'image_classification', 'version': '1.0.0'}\n"
     ]
    }
   ],
   "source": [
    "# Update\n",
    "dataset_information = {\"name\":\"Siemens Eval dataset\",\n",
    "                       \"description\":\"S eval dataset\"}\n",
    "data = json.dumps(dataset_information)\n",
    "\n",
    "endpoint = f\"{base_url}/datasets/{eval_dataset_id}\"\n",
    "\n",
    "response = requests.patch(endpoint, data=data, headers=headers)\n",
    "assert response.status_code in (200, 201)\n",
    "\n",
    "print(response)\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "432e3084",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading 1/1 tar split\n",
      "<Response [201]>\n",
      "{'message': 'Server recieved file and upload process started'}\n"
     ]
    }
   ],
   "source": [
    "# Upload\n",
    "output_dir = os.path.join(os.path.dirname(os.path.abspath(eval_dataset_path)), model_name, \"eval\")\n",
    "split_tar_file(eval_dataset_path, output_dir)\n",
    "for idx, tar_dataset_path in enumerate(os.listdir(output_dir)):\n",
    "    print(f\"Uploading {idx+1}/{len(os.listdir(output_dir))} tar split\")\n",
    "    files = [(\"file\",open(os.path.join(output_dir, tar_dataset_path),\"rb\"))]\n",
    "\n",
    "    endpoint = f\"{base_url}/datasets/{eval_dataset_id}:upload\"\n",
    "\n",
    "    response = requests.post(endpoint, files=files, headers=headers)\n",
    "    assert response.status_code in (200, 201)\n",
    "    assert \"message\" in response.json().keys() and response.json()[\"message\"] == \"Server recieved file and upload process started\"\n",
    "\n",
    "    print(response)\n",
    "    print(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53605661",
   "metadata": {},
   "source": [
    "### Create and upload test dataset <a class=\"anchor\" id=\"head-1.4\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "id": "56844fb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    {\n",
      "        \"actions\": [\n",
      "            \"train\",\n",
      "            \"evaluate\",\n",
      "            \"prune\",\n",
      "            \"retrain\",\n",
      "            \"export\",\n",
      "            \"gen_trt_engine\",\n",
      "            \"inference\"\n",
      "        ],\n",
      "        \"additional_id_info\": null,\n",
      "        \"automl_add_hyperparameters\": \"[]\",\n",
      "        \"automl_algorithm\": null,\n",
      "        \"automl_enabled\": false,\n",
      "        \"automl_remove_hyperparameters\": \"[]\",\n",
      "        \"base_experiment\": [],\n",
      "        \"calibration_dataset\": \"1c63db8b-5281-4aea-95dd-8f7b029039dc\",\n",
      "        \"checkpoint_choose_method\": \"best_model\",\n",
      "        \"checkpoint_epoch_number\": {},\n",
      "        \"created_on\": \"2024-08-21T05:42:01.537074\",\n",
      "        \"dataset_type\": \"image_classification\",\n",
      "        \"description\": \"My Experiments\",\n",
      "        \"docker_env_vars\": {},\n",
      "        \"encryption_key\": \"nvidia_tlt\",\n",
      "        \"eval_dataset\": \"d79beccd-32d3-4c7b-a936-4574f52ec94b\",\n",
      "        \"id\": \"d8ac076a-6bef-48cb-9dee-21e24d7cd615\",\n",
      "        \"inference_dataset\": \"aa022042-11c4-44c7-968d-0a5301059500\",\n",
      "        \"is_ptm_backbone\": true,\n",
      "        \"last_modified\": \"2024-08-21T05:42:01.537090\",\n",
      "        \"logo\": \"https://www.nvidia.com\",\n",
      "        \"metric\": null,\n",
      "        \"model_params\": {},\n",
      "        \"name\": \"My Experiment\",\n",
      "        \"network_arch\": \"classification_tf2\",\n",
      "        \"ngc_path\": \"\",\n",
      "        \"public\": false,\n",
      "        \"read_only\": false,\n",
      "        \"realtime_infer\": false,\n",
      "        \"realtime_infer_request_timeout\": 60,\n",
      "        \"realtime_infer_support\": false,\n",
      "        \"train_datasets\": [\n",
      "            \"1c63db8b-5281-4aea-95dd-8f7b029039dc\"\n",
      "        ],\n",
      "        \"type\": \"vision\",\n",
      "        \"version\": \"1.0.0\"\n",
      "    },\n",
      "    {\n",
      "        \"actions\": [\n",
      "            \"train\",\n",
      "            \"evaluate\",\n",
      "            \"prune\",\n",
      "            \"retrain\",\n",
      "            \"export\",\n",
      "            \"gen_trt_engine\",\n",
      "            \"inference\"\n",
      "        ],\n",
      "        \"additional_id_info\": null,\n",
      "        \"automl_R\": 15,\n",
      "        \"automl_add_hyperparameters\": \"[]\",\n",
      "        \"automl_algorithm\": \"hyperband\",\n",
      "        \"automl_enabled\": true,\n",
      "        \"automl_max_recommendations\": 20,\n",
      "        \"automl_nu\": 4,\n",
      "        \"automl_remove_hyperparameters\": \"[]\",\n",
      "        \"base_experiment\": [\n",
      "            \"cd5a8d37-132f-5811-b84c-c16ffe61b9e3\"\n",
      "        ],\n",
      "        \"calibration_dataset\": \"3a87ca28-3129-4f80-9a5a-2800434acace\",\n",
      "        \"checkpoint_choose_method\": \"best_model\",\n",
      "        \"checkpoint_epoch_number\": {},\n",
      "        \"created_on\": \"2024-08-21T02:29:20.079556\",\n",
      "        \"dataset_type\": \"image_classification\",\n",
      "        \"description\": \"My Experiments\",\n",
      "        \"docker_env_vars\": {},\n",
      "        \"encryption_key\": \"nvidia_tlt\",\n",
      "        \"eval_dataset\": \"1e9c8565-7e5c-42e7-a137-e811f1e05ddb\",\n",
      "        \"id\": \"7cfe7c67-27a9-4fb7-8bd1-983c10be635e\",\n",
      "        \"inference_dataset\": \"75d2d437-b225-410e-ab66-9eb7e769cc3a\",\n",
      "        \"is_ptm_backbone\": true,\n",
      "        \"last_modified\": \"2024-08-21T02:56:48.057567\",\n",
      "        \"logo\": \"https://www.nvidia.com\",\n",
      "        \"metric\": \"kpi\",\n",
      "        \"model_params\": {},\n",
      "        \"name\": \"My Experiment\",\n",
      "        \"network_arch\": \"classification_tf2\",\n",
      "        \"ngc_path\": \"\",\n",
      "        \"public\": false,\n",
      "        \"read_only\": false,\n",
      "        \"realtime_infer\": false,\n",
      "        \"realtime_infer_request_timeout\": 60,\n",
      "        \"realtime_infer_support\": false,\n",
      "        \"train_datasets\": [\n",
      "            \"3a87ca28-3129-4f80-9a5a-2800434acace\"\n",
      "        ],\n",
      "        \"type\": \"vision\",\n",
      "        \"version\": \"1.0.0\"\n",
      "    },\n",
      "    {\n",
      "        \"actions\": [\n",
      "            \"train\",\n",
      "            \"evaluate\",\n",
      "            \"prune\",\n",
      "            \"retrain\",\n",
      "            \"export\",\n",
      "            \"gen_trt_engine\",\n",
      "            \"inference\"\n",
      "        ],\n",
      "        \"additional_id_info\": null,\n",
      "        \"automl_R\": 15,\n",
      "        \"automl_add_hyperparameters\": \"[]\",\n",
      "        \"automl_algorithm\": \"hyperband\",\n",
      "        \"automl_enabled\": true,\n",
      "        \"automl_max_recommendations\": 20,\n",
      "        \"automl_nu\": 4,\n",
      "        \"automl_remove_hyperparameters\": \"[]\",\n",
      "        \"base_experiment\": [\n",
      "            \"cd5a8d37-132f-5811-b84c-c16ffe61b9e3\"\n",
      "        ],\n",
      "        \"calibration_dataset\": \"7998bcf9-1e40-4e3c-9b6c-675b60d24e6d\",\n",
      "        \"checkpoint_choose_method\": \"latest_model\",\n",
      "        \"checkpoint_epoch_number\": {\n",
      "            \"best_model_9e9cc34b-a96e-4eab-a2b2-288e5bd941cc\": 3,\n",
      "            \"latest_model_9e9cc34b-a96e-4eab-a2b2-288e5bd941cc\": 3\n",
      "        },\n",
      "        \"created_on\": \"2024-08-20T08:52:14.162214\",\n",
      "        \"dataset_type\": \"image_classification\",\n",
      "        \"description\": \"My Experiments\",\n",
      "        \"docker_env_vars\": {},\n",
      "        \"encryption_key\": \"nvidia_tlt\",\n",
      "        \"eval_dataset\": \"112b79fa-e93e-4487-89a1-43b5954fdf71\",\n",
      "        \"id\": \"a92eff4a-2c58-48d2-8cde-6499d3c0c939\",\n",
      "        \"inference_dataset\": \"2e94065b-c972-4002-98b9-480c09d8f98a\",\n",
      "        \"is_ptm_backbone\": true,\n",
      "        \"last_modified\": \"2024-08-20T09:09:14.079003\",\n",
      "        \"logo\": \"https://www.nvidia.com\",\n",
      "        \"metric\": \"kpi\",\n",
      "        \"model_params\": {},\n",
      "        \"name\": \"My Experiment\",\n",
      "        \"network_arch\": \"classification_tf2\",\n",
      "        \"ngc_path\": \"\",\n",
      "        \"public\": false,\n",
      "        \"read_only\": false,\n",
      "        \"realtime_infer\": false,\n",
      "        \"realtime_infer_request_timeout\": 60,\n",
      "        \"realtime_infer_support\": false,\n",
      "        \"train_datasets\": [\n",
      "            \"7998bcf9-1e40-4e3c-9b6c-675b60d24e6d\"\n",
      "        ],\n",
      "        \"type\": \"vision\",\n",
      "        \"version\": \"1.0.0\"\n",
      "    },\n",
      "    {\n",
      "        \"actions\": [\n",
      "            \"train\",\n",
      "            \"evaluate\",\n",
      "            \"prune\",\n",
      "            \"retrain\",\n",
      "            \"export\",\n",
      "            \"gen_trt_engine\",\n",
      "            \"inference\"\n",
      "        ],\n",
      "        \"additional_id_info\": null,\n",
      "        \"automl_R\": 15,\n",
      "        \"automl_add_hyperparameters\": \"[]\",\n",
      "        \"automl_algorithm\": \"hyperband\",\n",
      "        \"automl_enabled\": true,\n",
      "        \"automl_max_recommendations\": 20,\n",
      "        \"automl_nu\": 4,\n",
      "        \"automl_remove_hyperparameters\": \"[]\",\n",
      "        \"base_experiment\": [\n",
      "            \"cd5a8d37-132f-5811-b84c-c16ffe61b9e3\"\n",
      "        ],\n",
      "        \"calibration_dataset\": \"eff303f6-2d9e-412f-ba76-9a21d4ef0c11\",\n",
      "        \"checkpoint_choose_method\": \"best_model\",\n",
      "        \"checkpoint_epoch_number\": {},\n",
      "        \"created_on\": \"2024-08-20T08:28:21.119074\",\n",
      "        \"dataset_type\": \"image_classification\",\n",
      "        \"description\": \"My Experiments\",\n",
      "        \"docker_env_vars\": {},\n",
      "        \"encryption_key\": \"nvidia_tlt\",\n",
      "        \"eval_dataset\": \"b5806910-9273-48b2-b30c-0fea78e6a7d8\",\n",
      "        \"id\": \"e7d878d7-ab21-4890-8d9f-76ade7be1c2a\",\n",
      "        \"inference_dataset\": \"7162202a-aa48-43c6-a2dd-4a5a1c0e5976\",\n",
      "        \"is_ptm_backbone\": true,\n",
      "        \"last_modified\": \"2024-08-20T08:30:01.971154\",\n",
      "        \"logo\": \"https://www.nvidia.com\",\n",
      "        \"metric\": \"kpi\",\n",
      "        \"model_params\": {},\n",
      "        \"name\": \"My Experiment\",\n",
      "        \"network_arch\": \"classification_tf2\",\n",
      "        \"ngc_path\": \"\",\n",
      "        \"public\": false,\n",
      "        \"read_only\": false,\n",
      "        \"realtime_infer\": false,\n",
      "        \"realtime_infer_request_timeout\": 60,\n",
      "        \"realtime_infer_support\": false,\n",
      "        \"train_datasets\": [\n",
      "            \"eff303f6-2d9e-412f-ba76-9a21d4ef0c11\"\n",
      "        ],\n",
      "        \"type\": \"vision\",\n",
      "        \"version\": \"1.0.0\"\n",
      "    },\n",
      "    {\n",
      "        \"actions\": [\n",
      "            \"train\",\n",
      "            \"evaluate\",\n",
      "            \"prune\",\n",
      "            \"retrain\",\n",
      "            \"export\",\n",
      "            \"gen_trt_engine\",\n",
      "            \"inference\"\n",
      "        ],\n",
      "        \"additional_id_info\": null,\n",
      "        \"base_experiment\": [],\n",
      "        \"base_experiment_pull_complete\": \"not_present\",\n",
      "        \"calibration_dataset\": null,\n",
      "        \"checkpoint_choose_method\": \"best_model\",\n",
      "        \"checkpoint_epoch_number\": {\n",
      "            \"id\": 0\n",
      "        },\n",
      "        \"created_on\": \"2023-07-17T21:17:23.706Z\",\n",
      "        \"dataset_type\": \"image_classification\",\n",
      "        \"description\": \"Pre-trained EfficientNet backbone weights trained on NvImageNet to facilitate transfer learning using TAO Toolkit.\",\n",
      "        \"eval_dataset\": null,\n",
      "        \"id\": \"82b75315-2ae3-514b-82b3-00e781a11517\",\n",
      "        \"inference_dataset\": null,\n",
      "        \"is_ptm_backbone\": true,\n",
      "        \"last_modified\": \"2024-04-26T00:20:04.356Z\",\n",
      "        \"logo\": \"https://www.nvidia.com\",\n",
      "        \"name\": \"TAO Pretrained NVImageNet Classification backbone\",\n",
      "        \"network_arch\": \"classification_tf2\",\n",
      "        \"ngc_path\": \"nvidia/tao/pretrained_efficientdet_tf2_nvimagenet:efficientnet-b4\",\n",
      "        \"public\": true,\n",
      "        \"read_only\": true,\n",
      "        \"realtime_infer_support\": false,\n",
      "        \"sha256_digest\": {},\n",
      "        \"train_datasets\": [],\n",
      "        \"type\": \"vision\",\n",
      "        \"version\": \"efficientnet-b4\"\n",
      "    },\n",
      "    {\n",
      "        \"actions\": [\n",
      "            \"train\",\n",
      "            \"evaluate\",\n",
      "            \"prune\",\n",
      "            \"retrain\",\n",
      "            \"export\",\n",
      "            \"gen_trt_engine\",\n",
      "            \"inference\"\n",
      "        ],\n",
      "        \"additional_id_info\": null,\n",
      "        \"base_experiment\": [],\n",
      "        \"base_experiment_pull_complete\": \"not_present\",\n",
      "        \"calibration_dataset\": null,\n",
      "        \"checkpoint_choose_method\": \"best_model\",\n",
      "        \"checkpoint_epoch_number\": {\n",
      "            \"id\": 0\n",
      "        },\n",
      "        \"created_on\": \"2023-07-17T21:18:27.971Z\",\n",
      "        \"dataset_type\": \"image_classification\",\n",
      "        \"description\": \"Pre-trained EfficientNet backbone weights trained on NvImageNet to facilitate transfer learning using TAO Toolkit.\",\n",
      "        \"eval_dataset\": null,\n",
      "        \"id\": \"83f99c5a-cf58-53e5-9cc0-1185f30007c0\",\n",
      "        \"inference_dataset\": null,\n",
      "        \"is_ptm_backbone\": true,\n",
      "        \"last_modified\": \"2024-04-26T00:20:04.356Z\",\n",
      "        \"logo\": \"https://www.nvidia.com\",\n",
      "        \"name\": \"TAO Pretrained NVImageNet Classification backbone\",\n",
      "        \"network_arch\": \"classification_tf2\",\n",
      "        \"ngc_path\": \"nvidia/tao/pretrained_efficientdet_tf2_nvimagenet:efficientnet-b5\",\n",
      "        \"public\": true,\n",
      "        \"read_only\": true,\n",
      "        \"realtime_infer_support\": false,\n",
      "        \"sha256_digest\": {},\n",
      "        \"train_datasets\": [],\n",
      "        \"type\": \"vision\",\n",
      "        \"version\": \"efficientnet-b5\"\n",
      "    },\n",
      "    {\n",
      "        \"actions\": [\n",
      "            \"train\",\n",
      "            \"evaluate\",\n",
      "            \"prune\",\n",
      "            \"retrain\",\n",
      "            \"export\",\n",
      "            \"gen_trt_engine\",\n",
      "            \"inference\"\n",
      "        ],\n",
      "        \"additional_id_info\": null,\n",
      "        \"base_experiment\": [],\n",
      "        \"base_experiment_pull_complete\": \"not_present\",\n",
      "        \"calibration_dataset\": null,\n",
      "        \"checkpoint_choose_method\": \"best_model\",\n",
      "        \"checkpoint_epoch_number\": {\n",
      "            \"id\": 0\n",
      "        },\n",
      "        \"created_on\": \"2023-07-17T21:19:58.613Z\",\n",
      "        \"dataset_type\": \"image_classification\",\n",
      "        \"description\": \"Pre-trained EfficientNet backbone weights trained on NvImageNet to facilitate transfer learning using TAO Toolkit.\",\n",
      "        \"eval_dataset\": null,\n",
      "        \"id\": \"67e24d03-2ca2-59cd-b654-ec203303a339\",\n",
      "        \"inference_dataset\": null,\n",
      "        \"is_ptm_backbone\": true,\n",
      "        \"last_modified\": \"2024-04-26T00:20:04.356Z\",\n",
      "        \"logo\": \"https://www.nvidia.com\",\n",
      "        \"name\": \"TAO Pretrained NVImageNet Classification backbone\",\n",
      "        \"network_arch\": \"classification_tf2\",\n",
      "        \"ngc_path\": \"nvidia/tao/pretrained_efficientdet_tf2_nvimagenet:efficientnet-b3\",\n",
      "        \"public\": true,\n",
      "        \"read_only\": true,\n",
      "        \"realtime_infer_support\": false,\n",
      "        \"sha256_digest\": {},\n",
      "        \"train_datasets\": [],\n",
      "        \"type\": \"vision\",\n",
      "        \"version\": \"efficientnet-b3\"\n",
      "    },\n",
      "    {\n",
      "        \"actions\": [\n",
      "            \"train\",\n",
      "            \"evaluate\",\n",
      "            \"prune\",\n",
      "            \"retrain\",\n",
      "            \"export\",\n",
      "            \"gen_trt_engine\",\n",
      "            \"inference\"\n",
      "        ],\n",
      "        \"additional_id_info\": null,\n",
      "        \"base_experiment\": [],\n",
      "        \"base_experiment_pull_complete\": \"not_present\",\n",
      "        \"calibration_dataset\": null,\n",
      "        \"checkpoint_choose_method\": \"best_model\",\n",
      "        \"checkpoint_epoch_number\": {\n",
      "            \"id\": 0\n",
      "        },\n",
      "        \"created_on\": \"2023-07-17T21:20:48.505Z\",\n",
      "        \"dataset_type\": \"image_classification\",\n",
      "        \"description\": \"Pre-trained EfficientNet backbone weights trained on NvImageNet to facilitate transfer learning using TAO Toolkit.\",\n",
      "        \"eval_dataset\": null,\n",
      "        \"id\": \"4b9bc192-924e-5d49-895a-e55db53a6b4f\",\n",
      "        \"inference_dataset\": null,\n",
      "        \"is_ptm_backbone\": true,\n",
      "        \"last_modified\": \"2024-04-26T00:20:04.356Z\",\n",
      "        \"logo\": \"https://www.nvidia.com\",\n",
      "        \"name\": \"TAO Pretrained NVImageNet Classification backbone\",\n",
      "        \"network_arch\": \"classification_tf2\",\n",
      "        \"ngc_path\": \"nvidia/tao/pretrained_efficientdet_tf2_nvimagenet:efficientnet-b2\",\n",
      "        \"public\": true,\n",
      "        \"read_only\": true,\n",
      "        \"realtime_infer_support\": false,\n",
      "        \"sha256_digest\": {},\n",
      "        \"train_datasets\": [],\n",
      "        \"type\": \"vision\",\n",
      "        \"version\": \"efficientnet-b2\"\n",
      "    },\n",
      "    {\n",
      "        \"actions\": [\n",
      "            \"train\",\n",
      "            \"evaluate\",\n",
      "            \"prune\",\n",
      "            \"retrain\",\n",
      "            \"export\",\n",
      "            \"gen_trt_engine\",\n",
      "            \"inference\"\n",
      "        ],\n",
      "        \"additional_id_info\": null,\n",
      "        \"base_experiment\": [],\n",
      "        \"base_experiment_pull_complete\": \"not_present\",\n",
      "        \"calibration_dataset\": null,\n",
      "        \"checkpoint_choose_method\": \"best_model\",\n",
      "        \"checkpoint_epoch_number\": {\n",
      "            \"id\": 0\n",
      "        },\n",
      "        \"created_on\": \"2023-07-17T21:21:35.009Z\",\n",
      "        \"dataset_type\": \"image_classification\",\n",
      "        \"description\": \"Pre-trained EfficientNet backbone weights trained on NvImageNet to facilitate transfer learning using TAO Toolkit.\",\n",
      "        \"eval_dataset\": null,\n",
      "        \"id\": \"6f64702e-624a-57e6-bba4-9809eddb6ab0\",\n",
      "        \"inference_dataset\": null,\n",
      "        \"is_ptm_backbone\": true,\n",
      "        \"last_modified\": \"2024-04-26T00:20:04.356Z\",\n",
      "        \"logo\": \"https://www.nvidia.com\",\n",
      "        \"name\": \"TAO Pretrained NVImageNet Classification backbone\",\n",
      "        \"network_arch\": \"classification_tf2\",\n",
      "        \"ngc_path\": \"nvidia/tao/pretrained_efficientdet_tf2_nvimagenet:efficientnet-b1\",\n",
      "        \"public\": true,\n",
      "        \"read_only\": true,\n",
      "        \"realtime_infer_support\": false,\n",
      "        \"sha256_digest\": {},\n",
      "        \"train_datasets\": [],\n",
      "        \"type\": \"vision\",\n",
      "        \"version\": \"efficientnet-b1\"\n",
      "    },\n",
      "    {\n",
      "        \"actions\": [\n",
      "            \"train\",\n",
      "            \"evaluate\",\n",
      "            \"prune\",\n",
      "            \"retrain\",\n",
      "            \"export\",\n",
      "            \"gen_trt_engine\",\n",
      "            \"inference\"\n",
      "        ],\n",
      "        \"additional_id_info\": null,\n",
      "        \"base_experiment\": [],\n",
      "        \"base_experiment_pull_complete\": \"not_present\",\n",
      "        \"calibration_dataset\": null,\n",
      "        \"checkpoint_choose_method\": \"best_model\",\n",
      "        \"checkpoint_epoch_number\": {\n",
      "            \"id\": 0\n",
      "        },\n",
      "        \"created_on\": \"2023-07-17T21:22:24.168Z\",\n",
      "        \"dataset_type\": \"image_classification\",\n",
      "        \"description\": \"Pre-trained EfficientNet backbone weights trained on NvImageNet to facilitate transfer learning using TAO Toolkit.\",\n",
      "        \"eval_dataset\": null,\n",
      "        \"id\": \"7b0c0576-92b3-5585-9659-de8d419bfddc\",\n",
      "        \"inference_dataset\": null,\n",
      "        \"is_ptm_backbone\": true,\n",
      "        \"last_modified\": \"2024-04-26T00:20:04.356Z\",\n",
      "        \"logo\": \"https://www.nvidia.com\",\n",
      "        \"name\": \"TAO Pretrained NVImageNet Classification backbone\",\n",
      "        \"network_arch\": \"classification_tf2\",\n",
      "        \"ngc_path\": \"nvidia/tao/pretrained_efficientdet_tf2_nvimagenet:efficientnet-b0\",\n",
      "        \"public\": true,\n",
      "        \"read_only\": true,\n",
      "        \"realtime_infer_support\": false,\n",
      "        \"sha256_digest\": {},\n",
      "        \"train_datasets\": [],\n",
      "        \"type\": \"vision\",\n",
      "        \"version\": \"efficientnet-b0\"\n",
      "    },\n",
      "    {\n",
      "        \"actions\": [\n",
      "            \"train\",\n",
      "            \"evaluate\",\n",
      "            \"prune\",\n",
      "            \"retrain\",\n",
      "            \"export\",\n",
      "            \"gen_trt_engine\",\n",
      "            \"inference\"\n",
      "        ],\n",
      "        \"additional_id_info\": null,\n",
      "        \"base_experiment\": [],\n",
      "        \"base_experiment_pull_complete\": \"present\",\n",
      "        \"calibration_dataset\": null,\n",
      "        \"checkpoint_choose_method\": \"best_model\",\n",
      "        \"checkpoint_epoch_number\": {\n",
      "            \"id\": 0\n",
      "        },\n",
      "        \"created_on\": \"2022-12-08T23:34:10.884Z\",\n",
      "        \"dataset_type\": \"image_classification\",\n",
      "        \"description\": \"Pretrained backbones for TAO Toolkit TF2 image classification\",\n",
      "        \"eval_dataset\": null,\n",
      "        \"id\": \"cd5a8d37-132f-5811-b84c-c16ffe61b9e3\",\n",
      "        \"inference_dataset\": null,\n",
      "        \"is_ptm_backbone\": true,\n",
      "        \"last_modified\": \"2022-12-08T23:40:30.534Z\",\n",
      "        \"logo\": \"https://www.nvidia.com\",\n",
      "        \"name\": \"TAO Pretrained Classification\",\n",
      "        \"network_arch\": \"classification_tf2\",\n",
      "        \"ngc_path\": \"nvidia/tao/pretrained_classification_tf2:efficientnet_b0\",\n",
      "        \"public\": true,\n",
      "        \"read_only\": true,\n",
      "        \"realtime_infer_support\": false,\n",
      "        \"sha256_digest\": {},\n",
      "        \"train_datasets\": [],\n",
      "        \"type\": \"vision\",\n",
      "        \"version\": \"efficientnet_b0\"\n",
      "    }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# Assuming the JSON object is stored in the variable 'response_json'\n",
    "pretty_json = json.dumps(response_json, indent=4)\n",
    "\n",
    "print(pretty_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "53ca504d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [201]>\n",
      "{'actions': [], 'client_url': None, 'created_on': '2024-08-21T05:39:43.333864', 'description': 'My TAO Dataset', 'docker_env_vars': {}, 'format': 'default', 'id': 'aa022042-11c4-44c7-968d-0a5301059500', 'jobs': [], 'last_modified': '2024-08-21T05:39:43.333874', 'logo': 'https://www.nvidia.com', 'name': 'My Dataset', 'pull': None, 'status': 'not_present', 'type': 'image_classification', 'version': '1.0.0'}\n"
     ]
    }
   ],
   "source": [
    " # Create testing dataset for inference\n",
    "ds_type = \"image_classification\"\n",
    "ds_format = \"default\"\n",
    "data = json.dumps({\"type\":ds_type,\"format\":ds_format})\n",
    "\n",
    "endpoint = f\"{base_url}/datasets\"\n",
    "\n",
    "response = requests.post(endpoint,data=data,headers=headers)\n",
    "assert response.status_code in (200, 201)\n",
    "\n",
    "print(response)\n",
    "print(response.json())\n",
    "assert \"id\" in response.json().keys()\n",
    "test_dataset_id = response.json()[\"id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "bc15c48b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading 1/1 tar split\n",
      "<Response [201]>\n",
      "{'message': 'Server recieved file and upload process started'}\n"
     ]
    }
   ],
   "source": [
    "# Upload\n",
    "output_dir = os.path.join(os.path.dirname(os.path.abspath(test_dataset_path)), model_name, \"test\")\n",
    "split_tar_file(test_dataset_path, output_dir)\n",
    "for idx, tar_dataset_path in enumerate(os.listdir(output_dir)):\n",
    "    print(f\"Uploading {idx+1}/{len(os.listdir(output_dir))} tar split\")\n",
    "    files = [(\"file\",open(os.path.join(output_dir, tar_dataset_path),\"rb\"))]\n",
    "\n",
    "    endpoint = f\"{base_url}/datasets/{test_dataset_id}:upload\"\n",
    "\n",
    "    response = requests.post(endpoint, files=files, headers=headers)\n",
    "    assert response.status_code in (200, 201)\n",
    "    assert \"message\" in response.json().keys() and response.json()[\"message\"] == \"Server recieved file and upload process started\"\n",
    "\n",
    "    print(response)\n",
    "    print(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9b088b",
   "metadata": {},
   "source": [
    "### List the created datasets <a class=\"anchor\" id=\"head-2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "d62f7ec8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n",
      "id\t\t\t\t\t type\t\t\t format\t\t name\n",
      "aa022042-11c4-44c7-968d-0a5301059500 \t image_classification \t default \t\t My Dataset\n",
      "b43ccf4a-90cc-4a7e-a75d-a7baf303a040 \t image_classification \t raw \t\t My Dataset\n",
      "d79beccd-32d3-4c7b-a936-4574f52ec94b \t image_classification \t default \t\t Siemens Eval dataset\n",
      "1c63db8b-5281-4aea-95dd-8f7b029039dc \t image_classification \t default \t\t Siemens Train Dataset\n",
      "75d2d437-b225-410e-ab66-9eb7e769cc3a \t image_classification \t raw \t\t My Dataset\n",
      "1e9c8565-7e5c-42e7-a137-e811f1e05ddb \t image_classification \t default \t\t Siemens Eval dataset\n",
      "3a87ca28-3129-4f80-9a5a-2800434acace \t image_classification \t default \t\t Siemens Train Dataset\n",
      "2e94065b-c972-4002-98b9-480c09d8f98a \t image_classification \t raw \t\t My Dataset\n",
      "112b79fa-e93e-4487-89a1-43b5954fdf71 \t image_classification \t default \t\t Siemens Eval dataset\n",
      "7998bcf9-1e40-4e3c-9b6c-675b60d24e6d \t image_classification \t default \t\t Siemens Train Dataset\n",
      "7162202a-aa48-43c6-a2dd-4a5a1c0e5976 \t image_classification \t raw \t\t My Dataset\n",
      "b5806910-9273-48b2-b30c-0fea78e6a7d8 \t image_classification \t default \t\t Siemens Eval dataset\n",
      "b8d7b7fb-b398-4ca6-aee5-fe58028ce0c8 \t image_classification \t raw \t\t My Dataset\n",
      "7c878403-9f05-432f-bdc2-9efad3522717 \t image_classification \t default \t\t Eval dataset\n",
      "eff303f6-2d9e-412f-ba76-9a21d4ef0c11 \t image_classification \t default \t\t Siemens Train Dataset\n",
      "4d10181b-31a7-4576-8fd9-282f7ae0c4a8 \t image_classification \t default \t\t Train dataset\n",
      "2e0db5b1-9aa4-4fea-9e32-50104c62d1ab \t image_classification \t default \t\t My Dataset\n",
      "e8cdd785-b856-4f2b-a602-0d00e3cd046f \t image_classification \t raw \t\t My Dataset\n",
      "c199add8-2e07-4c92-8b77-442afac11a56 \t image_classification \t default \t\t Eval dataset\n",
      "f44b9606-2ede-4fe2-b9dd-52f53b2084e3 \t image_classification \t custom \t\t Train dataset\n",
      "e512d1da-6be7-457d-a052-5d98ee24df54 \t object_detection \t raw \t\t My Dataset\n",
      "2784cafa-874c-453f-9686-e2289a5b2b90 \t object_detection \t kitti \t\t Eval dataset\n",
      "7477f92d-be3d-4995-b021-06877c8353f8 \t object_detection \t kitti \t\t Train dataset\n"
     ]
    }
   ],
   "source": [
    "endpoint = f\"{base_url}/datasets\"\n",
    "\n",
    "response = requests.get(endpoint, headers=headers)\n",
    "assert response.status_code in (200, 201)\n",
    "\n",
    "print(response)\n",
    "# print(response.json()) ## Uncomment for verbose list output\n",
    "print(\"id\\t\\t\\t\\t\\t type\\t\\t\\t format\\t\\t name\")\n",
    "for rsp in response.json():\n",
    "    rsp_keys = rsp.keys()\n",
    "    assert \"id\" in rsp_keys\n",
    "    assert \"type\" in rsp_keys\n",
    "    assert \"format\" in rsp_keys\n",
    "    assert \"name\" in rsp_keys\n",
    "    print(rsp[\"id\"],\"\\t\",rsp[\"type\"],\"\\t\",rsp[\"format\"],\"\\t\\t\",rsp[\"name\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87f508e",
   "metadata": {},
   "source": [
    "### Create an experiment <a class=\"anchor\" id=\"head-4\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "id": "e03aba62",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [201]>\n",
      "{'actions': ['train', 'evaluate', 'prune', 'retrain', 'export', 'gen_trt_engine', 'inference'], 'additional_id_info': None, 'automl_add_hyperparameters': '[]', 'automl_algorithm': None, 'automl_enabled': False, 'automl_remove_hyperparameters': '[]', 'base_experiment': [], 'calibration_dataset': None, 'checkpoint_choose_method': 'best_model', 'checkpoint_epoch_number': {}, 'created_on': '2024-08-21T06:43:41.391967', 'dataset_type': 'image_classification', 'description': 'My Experiments', 'docker_env_vars': {}, 'encryption_key': 'nvidia_tlt', 'eval_dataset': None, 'id': 'd862a1b9-1305-4ac9-9d1f-a9e94e2b9360', 'inference_dataset': None, 'is_ptm_backbone': True, 'jobs': [], 'last_modified': '2024-08-21T06:43:41.391984', 'logo': 'https://www.nvidia.com', 'metric': None, 'model_params': {}, 'name': 'My Experiment', 'network_arch': 'classification_tf2', 'ngc_path': '', 'public': False, 'read_only': False, 'realtime_infer': False, 'realtime_infer_request_timeout': 60, 'realtime_infer_support': False, 'train_datasets': [], 'type': 'vision', 'version': '1.0.0'}\n"
     ]
    }
   ],
   "source": [
    "if \"classification\" in model_name:\n",
    "    encode_key = \"nvidia_tlt\"\n",
    "else:\n",
    "    encode_key = \"tlt_encode\"\n",
    "checkpoint_choose_method = \"best_model\"\n",
    "data = json.dumps({\"network_arch\":model_name,\"encryption_key\":encode_key,\"checkpoint_choose_method\":checkpoint_choose_method})\n",
    "\n",
    "endpoint = f\"{base_url}/experiments\"\n",
    "\n",
    "response = requests.post(endpoint,data=data,headers=headers)\n",
    "assert response.status_code in (200, 201)\n",
    "\n",
    "print(response)\n",
    "print(response.json())\n",
    "assert \"id\" in response.json().keys()\n",
    "experiment_id = response.json()[\"id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "id": "2e77223a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'actions': ['train',\n",
       "  'evaluate',\n",
       "  'prune',\n",
       "  'retrain',\n",
       "  'export',\n",
       "  'gen_trt_engine',\n",
       "  'inference'],\n",
       " 'additional_id_info': None,\n",
       " 'automl_add_hyperparameters': '[]',\n",
       " 'automl_algorithm': None,\n",
       " 'automl_enabled': False,\n",
       " 'automl_remove_hyperparameters': '[]',\n",
       " 'base_experiment': [],\n",
       " 'calibration_dataset': None,\n",
       " 'checkpoint_choose_method': 'best_model',\n",
       " 'checkpoint_epoch_number': {},\n",
       " 'created_on': '2024-08-21T06:43:41.391967',\n",
       " 'dataset_type': 'image_classification',\n",
       " 'description': 'My Experiments',\n",
       " 'docker_env_vars': {},\n",
       " 'encryption_key': 'nvidia_tlt',\n",
       " 'eval_dataset': None,\n",
       " 'id': 'd862a1b9-1305-4ac9-9d1f-a9e94e2b9360',\n",
       " 'inference_dataset': None,\n",
       " 'is_ptm_backbone': True,\n",
       " 'jobs': [],\n",
       " 'last_modified': '2024-08-21T06:43:41.391984',\n",
       " 'logo': 'https://www.nvidia.com',\n",
       " 'metric': None,\n",
       " 'model_params': {},\n",
       " 'name': 'My Experiment',\n",
       " 'network_arch': 'classification_tf2',\n",
       " 'ngc_path': '',\n",
       " 'public': False,\n",
       " 'read_only': False,\n",
       " 'realtime_infer': False,\n",
       " 'realtime_infer_request_timeout': 60,\n",
       " 'realtime_infer_support': False,\n",
       " 'train_datasets': [],\n",
       " 'type': 'vision',\n",
       " 'version': '1.0.0'}"
      ]
     },
     "execution_count": 364,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf48d9e",
   "metadata": {},
   "source": [
    "### List experiments <a class=\"anchor\" id=\"head-5\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "id": "2ae83f46",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n",
      "model id\t\t\t     network architecture\n",
      "My Experiment d862a1b9-1305-4ac9-9d1f-a9e94e2b9360 classification_tf2\n",
      "My Experiment 9b2e365c-8726-4ef6-8b15-f0aa9a047476 classification_tf2\n",
      "My Experiment d8ac076a-6bef-48cb-9dee-21e24d7cd615 classification_tf2\n",
      "My Experiment 7cfe7c67-27a9-4fb7-8bd1-983c10be635e classification_tf2\n",
      "My Experiment a92eff4a-2c58-48d2-8cde-6499d3c0c939 classification_tf2\n",
      "My Experiment e7d878d7-ab21-4890-8d9f-76ade7be1c2a classification_tf2\n",
      "TAO Pretrained NVImageNet Classification backbone 82b75315-2ae3-514b-82b3-00e781a11517 classification_tf2\n",
      "TAO Pretrained NVImageNet Classification backbone 83f99c5a-cf58-53e5-9cc0-1185f30007c0 classification_tf2\n",
      "TAO Pretrained NVImageNet Classification backbone 67e24d03-2ca2-59cd-b654-ec203303a339 classification_tf2\n",
      "TAO Pretrained NVImageNet Classification backbone 4b9bc192-924e-5d49-895a-e55db53a6b4f classification_tf2\n",
      "TAO Pretrained NVImageNet Classification backbone 6f64702e-624a-57e6-bba4-9809eddb6ab0 classification_tf2\n",
      "TAO Pretrained NVImageNet Classification backbone 7b0c0576-92b3-5585-9659-de8d419bfddc classification_tf2\n",
      "TAO Pretrained Classification cd5a8d37-132f-5811-b84c-c16ffe61b9e3 classification_tf2\n"
     ]
    }
   ],
   "source": [
    "endpoint = f\"{base_url}/experiments\"\n",
    "params = {\"network_arch\": model_name}\n",
    "response = requests.get(endpoint, params=params, headers=headers)\n",
    "assert response.status_code in (200, 201)\n",
    "\n",
    "print(response)\n",
    "# print(response.json()) ## Uncomment for verbose list output\n",
    "print(\"model id\\t\\t\\t     network architecture\")\n",
    "for rsp in response.json():\n",
    "    rsp_keys = rsp.keys()\n",
    "    assert \"id\" in rsp_keys and \"network_arch\" in rsp_keys\n",
    "    print(rsp[\"name\"], rsp[\"id\"],rsp[\"network_arch\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87862f08",
   "metadata": {},
   "source": [
    "### Assign train, eval datasets <a class=\"anchor\" id=\"head-6\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac03bec3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "id": "811a508a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_datasets': ['1c63db8b-5281-4aea-95dd-8f7b029039dc'], 'eval_dataset': 'd79beccd-32d3-4c7b-a936-4574f52ec94b', 'inference_dataset': 'aa022042-11c4-44c7-968d-0a5301059500', 'calibration_dataset': '1c63db8b-5281-4aea-95dd-8f7b029039dc'}\n",
      "<Response [200]>\n",
      "{'actions': ['train', 'evaluate', 'prune', 'retrain', 'export', 'gen_trt_engine', 'inference'], 'additional_id_info': None, 'automl_add_hyperparameters': '[]', 'automl_algorithm': None, 'automl_enabled': False, 'automl_remove_hyperparameters': '[]', 'base_experiment': [], 'calibration_dataset': '1c63db8b-5281-4aea-95dd-8f7b029039dc', 'checkpoint_choose_method': 'best_model', 'checkpoint_epoch_number': {}, 'created_on': '2024-08-21T06:43:41.391967', 'dataset_type': 'image_classification', 'description': 'My Experiments', 'docker_env_vars': {}, 'encryption_key': 'nvidia_tlt', 'eval_dataset': 'd79beccd-32d3-4c7b-a936-4574f52ec94b', 'id': 'd862a1b9-1305-4ac9-9d1f-a9e94e2b9360', 'inference_dataset': 'aa022042-11c4-44c7-968d-0a5301059500', 'is_ptm_backbone': True, 'jobs': [], 'last_modified': '2024-08-21T06:43:41.391984', 'logo': 'https://www.nvidia.com', 'metric': None, 'model_params': {}, 'name': 'My Experiment', 'network_arch': 'classification_tf2', 'ngc_path': '', 'public': False, 'read_only': False, 'realtime_infer': False, 'realtime_infer_request_timeout': 60, 'realtime_infer_support': False, 'train_datasets': ['1c63db8b-5281-4aea-95dd-8f7b029039dc'], 'type': 'vision', 'version': '1.0.0'}\n"
     ]
    }
   ],
   "source": [
    "dataset_information = {\"train_datasets\":[train_dataset_id],\n",
    "                       \"eval_dataset\":eval_dataset_id,\n",
    "                       \"inference_dataset\":test_dataset_id,\n",
    "                       \"calibration_dataset\":train_dataset_id}\n",
    "print(dataset_information)\n",
    "data = json.dumps(dataset_information)\n",
    "\n",
    "endpoint = f\"{base_url}/experiments/{experiment_id}\"\n",
    "\n",
    "response = requests.patch(endpoint, data=data, headers=headers)\n",
    "assert response.status_code in (200, 201)\n",
    "\n",
    "print(response)\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "9c8ae925",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d2c68a",
   "metadata": {},
   "source": [
    "### Assign PTM <a class=\"anchor\" id=\"head-7\"></a>\n",
    "\n",
    "Search for the PTM on NGC for the Classification model chosen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "1902078c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'network_arch': 'classification_tf2'}"
      ]
     },
     "execution_count": 333,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "id": "fc5a457d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PTM Name: TAO Pretrained NVImageNet Classification backbone; PTM version: efficientnet-b4; NGC PATH: nvidia/tao/pretrained_efficientdet_tf2_nvimagenet:efficientnet-b4; Additional info: None\n",
      "PTM Name: TAO Pretrained NVImageNet Classification backbone; PTM version: efficientnet-b5; NGC PATH: nvidia/tao/pretrained_efficientdet_tf2_nvimagenet:efficientnet-b5; Additional info: None\n",
      "PTM Name: TAO Pretrained NVImageNet Classification backbone; PTM version: efficientnet-b3; NGC PATH: nvidia/tao/pretrained_efficientdet_tf2_nvimagenet:efficientnet-b3; Additional info: None\n",
      "PTM Name: TAO Pretrained NVImageNet Classification backbone; PTM version: efficientnet-b2; NGC PATH: nvidia/tao/pretrained_efficientdet_tf2_nvimagenet:efficientnet-b2; Additional info: None\n",
      "PTM Name: TAO Pretrained NVImageNet Classification backbone; PTM version: efficientnet-b1; NGC PATH: nvidia/tao/pretrained_efficientdet_tf2_nvimagenet:efficientnet-b1; Additional info: None\n",
      "PTM Name: TAO Pretrained NVImageNet Classification backbone; PTM version: efficientnet-b0; NGC PATH: nvidia/tao/pretrained_efficientdet_tf2_nvimagenet:efficientnet-b0; Additional info: None\n",
      "PTM Name: TAO Pretrained Classification; PTM version: efficientnet_b0; NGC PATH: nvidia/tao/pretrained_classification_tf2:efficientnet_b0; Additional info: None\n"
     ]
    }
   ],
   "source": [
    "# List all pretrained models for the chosen network architecture\n",
    "endpoint = f\"{base_url}/experiments\"\n",
    "params = {\"network_arch\": model_name}\n",
    "response = requests.get(endpoint, params=params, headers=headers)\n",
    "assert response.status_code in (200, 201)\n",
    "\n",
    "response_json = response.json()\n",
    "\n",
    "for rsp in response_json:\n",
    "    rsp_keys = rsp.keys()\n",
    "    if \"encryption_key\" not in rsp.keys():\n",
    "        assert \"name\" in rsp_keys and \"version\" in rsp_keys and \"ngc_path\" in rsp_keys and \"additional_id_info\" in rsp_keys\n",
    "        print(f'PTM Name: {rsp[\"name\"]}; PTM version: {rsp[\"version\"]}; NGC PATH: {rsp[\"ngc_path\"]}; Additional info: {rsp[\"additional_id_info\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "id": "014070c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Assigning pretrained models to different classification models\n",
    "# From the output of previous cell make the appropriate changes to this map if you want to change the default PTM backbone.\n",
    "# Changing the default backbone here requires changing default spec/config during train/eval etc like for example\n",
    "# If you are changing the ptm to resnet34, then you have to modify the config key num_layers if it exists to 34 manually\n",
    "pretrained_map = {\"classification_tf1\" : \"pretrained_classification:resnet18\",\n",
    "                  \"classification_tf2\" : \"pretrained_classification_tf2:efficientnet_b0\",\n",
    "                  \"classification_pyt\" : \"pretrained_fan_classification_imagenet:fan_hybrid_tiny\",\n",
    "                  \"multitask_classification\" : \"pretrained_classification:resnet10\"}\n",
    "no_ptm_models = set([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "id": "1d10199d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata for model with requested NGC Path\n",
      "{'actions': ['train', 'evaluate', 'prune', 'retrain', 'export', 'gen_trt_engine', 'inference'], 'additional_id_info': None, 'base_experiment': [], 'base_experiment_pull_complete': 'present', 'calibration_dataset': None, 'checkpoint_choose_method': 'best_model', 'checkpoint_epoch_number': {'id': 0}, 'created_on': '2022-12-08T23:34:10.884Z', 'dataset_type': 'image_classification', 'description': 'Pretrained backbones for TAO Toolkit TF2 image classification', 'eval_dataset': None, 'id': 'cd5a8d37-132f-5811-b84c-c16ffe61b9e3', 'inference_dataset': None, 'is_ptm_backbone': True, 'last_modified': '2022-12-08T23:40:30.534Z', 'logo': 'https://www.nvidia.com', 'name': 'TAO Pretrained Classification', 'network_arch': 'classification_tf2', 'ngc_path': 'nvidia/tao/pretrained_classification_tf2:efficientnet_b0', 'public': True, 'read_only': True, 'realtime_infer_support': False, 'sha256_digest': {}, 'train_datasets': [], 'type': 'vision', 'version': 'efficientnet_b0'}\n"
     ]
    }
   ],
   "source": [
    "# Get pretrained model for classification\n",
    "if model_name not in no_ptm_models:\n",
    "    endpoint = f\"{base_url}/experiments\"\n",
    "    params = {\"network_arch\": model_name}\n",
    "    response = requests.get(endpoint, params=params, headers=headers)\n",
    "    assert response.status_code in (200, 201)\n",
    "\n",
    "    response_json = response.json()\n",
    "\n",
    "    # Search for ptm with given ngc path\n",
    "    ptm = []\n",
    "    for rsp in response_json:\n",
    "        assert \"ngc_path\" in rsp_keys\n",
    "        if rsp[\"ngc_path\"].endswith(pretrained_map[model_name]):\n",
    "            assert \"id\" in rsp_keys\n",
    "            ptm_id = rsp[\"id\"]\n",
    "            ptm = [ptm_id]\n",
    "            print(\"Metadata for model with requested NGC Path\")\n",
    "            print(rsp)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "id": "c3aadfc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'classification_tf2'"
      ]
     },
     "execution_count": 349,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "id": "7dab250e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"network_arch\": \"classification_tf2\", \"encryption_key\": \"nvidia_tlt\", \"checkpoint_choose_method\": \"best_model\"}'"
      ]
     },
     "execution_count": 350,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "id": "5c344b32",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n",
      "{'actions': ['train', 'evaluate', 'prune', 'retrain', 'export', 'gen_trt_engine', 'inference'], 'additional_id_info': None, 'automl_add_hyperparameters': '[]', 'automl_algorithm': None, 'automl_enabled': False, 'automl_remove_hyperparameters': '[]', 'base_experiment': ['cd5a8d37-132f-5811-b84c-c16ffe61b9e3'], 'calibration_dataset': '1c63db8b-5281-4aea-95dd-8f7b029039dc', 'checkpoint_choose_method': 'best_model', 'checkpoint_epoch_number': {}, 'created_on': '2024-08-21T06:43:41.391967', 'dataset_type': 'image_classification', 'description': 'My Experiments', 'docker_env_vars': {}, 'encryption_key': 'nvidia_tlt', 'eval_dataset': 'd79beccd-32d3-4c7b-a936-4574f52ec94b', 'id': 'd862a1b9-1305-4ac9-9d1f-a9e94e2b9360', 'inference_dataset': 'aa022042-11c4-44c7-968d-0a5301059500', 'is_ptm_backbone': True, 'jobs': [], 'last_modified': '2024-08-21T06:43:41.391984', 'logo': 'https://www.nvidia.com', 'metric': None, 'model_params': {}, 'name': 'My Experiment', 'network_arch': 'classification_tf2', 'ngc_path': '', 'public': False, 'read_only': False, 'realtime_infer': False, 'realtime_infer_request_timeout': 60, 'realtime_infer_support': False, 'train_datasets': ['1c63db8b-5281-4aea-95dd-8f7b029039dc'], 'type': 'vision', 'version': '1.0.0'}\n"
     ]
    }
   ],
   "source": [
    "if model_name not in no_ptm_models:\n",
    "    ptm_information = {\"base_experiment\":ptm}\n",
    "    data = json.dumps(ptm_information)\n",
    "\n",
    "    endpoint = f\"{base_url}/experiments/{experiment_id}\"\n",
    "\n",
    "    response = requests.patch(endpoint, data=data, headers=headers)\n",
    "    assert response.status_code in (200, 201)\n",
    "\n",
    "    print(response)\n",
    "    print(response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "id": "15cd75cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['train.reg_config.type',\n",
       " 'train.optim_config.lr',\n",
       " 'train.optim_config.beta_1',\n",
       " 'train.optim_config.nesterov']"
      ]
     },
     "execution_count": 353,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "automl_specs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8497146",
   "metadata": {},
   "source": [
    "### View hyperparameters that are enabled for AutoML by default <a class=\"anchor\" id=\"head-8\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "id": "3bb8e769",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    \"train.reg_config.type\",\n",
      "    \"train.optim_config.lr\",\n",
      "    \"train.optim_config.beta_1\",\n",
      "    \"train.optim_config.nesterov\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "if automl_enabled:\n",
    "    # Get default spec schema\n",
    "    endpoint = f\"{base_url}/experiments/{experiment_id}/specs/train/schema\"\n",
    "    response = requests.get(endpoint, headers=headers)\n",
    "    assert response.status_code in (200, 201)\n",
    "    assert \"automl_default_parameters\" in response.json().keys()\n",
    "    automl_specs = response.json()[\"automl_default_parameters\"]\n",
    "    print(json.dumps(automl_specs, sort_keys=True, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97965cbe",
   "metadata": {},
   "source": [
    "### Actions <a class=\"anchor\" id=\"head-10\"></a>\n",
    "\n",
    "For all actions:\n",
    "1. Get default spec schema and derive the default values\n",
    "2. Modify defaults if needed\n",
    "3. Post spec dictionary to the service\n",
    "4. Run model action\n",
    "5. Monitor job using retrieve\n",
    "6. Download results using job download endpoint (if needed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b85b897",
   "metadata": {},
   "source": [
    "### Train <a class=\"anchor\" id=\"head-11\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd05d8e",
   "metadata": {},
   "source": [
    "#### Set AutoML related configurations <a class=\"anchor\" id=\"head-9\"></a>\n",
    "Refer to these hyper-links to see the parameters supported by each network and add more parameters if necessary in addition to the default automl enabled parameters:\n",
    "\n",
    "[Classification TF1](https://github.com/NVIDIA/tao_front_end_services/tree/main/api/specs_utils/specs/classification_tf1/classification_tf1%20-%20train.csv), \n",
    "[Classification TF2](https://github.com/NVIDIA/tao_front_end_services/tree/main/api/specs_utils/specs/classification_tf2/classification_tf2%20-%20train.csv), \n",
    "[Classification Pytorch](https://github.com/NVIDIA/tao_front_end_services/tree/main/api/specs_utils/specs/classification_pyt/classification_pyt%20-%20train.csv), \n",
    "[Multitask classification](https://github.com/NVIDIA/tao_front_end_services/tree/main/api/specs_utils/specs/multitask_classification/multitask_classification%20-%20train.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "id": "a21f269d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n",
      "{\n",
      "    \"actions\": [\n",
      "        \"train\",\n",
      "        \"evaluate\",\n",
      "        \"prune\",\n",
      "        \"retrain\",\n",
      "        \"export\",\n",
      "        \"gen_trt_engine\",\n",
      "        \"inference\"\n",
      "    ],\n",
      "    \"additional_id_info\": null,\n",
      "    \"automl_R\": 15,\n",
      "    \"automl_add_hyperparameters\": \"[]\",\n",
      "    \"automl_algorithm\": \"bayesian\",\n",
      "    \"automl_enabled\": true,\n",
      "    \"automl_max_recommendations\": 20,\n",
      "    \"automl_nu\": 4,\n",
      "    \"automl_remove_hyperparameters\": \"[]\",\n",
      "    \"base_experiment\": [\n",
      "        \"cd5a8d37-132f-5811-b84c-c16ffe61b9e3\"\n",
      "    ],\n",
      "    \"calibration_dataset\": \"1c63db8b-5281-4aea-95dd-8f7b029039dc\",\n",
      "    \"checkpoint_choose_method\": \"best_model\",\n",
      "    \"checkpoint_epoch_number\": {},\n",
      "    \"created_on\": \"2024-08-21T06:43:41.391967\",\n",
      "    \"dataset_type\": \"image_classification\",\n",
      "    \"description\": \"My Experiments\",\n",
      "    \"docker_env_vars\": {},\n",
      "    \"encryption_key\": \"nvidia_tlt\",\n",
      "    \"eval_dataset\": \"d79beccd-32d3-4c7b-a936-4574f52ec94b\",\n",
      "    \"id\": \"d862a1b9-1305-4ac9-9d1f-a9e94e2b9360\",\n",
      "    \"inference_dataset\": \"aa022042-11c4-44c7-968d-0a5301059500\",\n",
      "    \"is_ptm_backbone\": true,\n",
      "    \"jobs\": [],\n",
      "    \"last_modified\": \"2024-08-21T06:48:42.198879\",\n",
      "    \"logo\": \"https://www.nvidia.com\",\n",
      "    \"metric\": \"kpi\",\n",
      "    \"model_params\": {},\n",
      "    \"name\": \"My Experiment\",\n",
      "    \"network_arch\": \"classification_tf2\",\n",
      "    \"ngc_path\": \"\",\n",
      "    \"public\": false,\n",
      "    \"read_only\": false,\n",
      "    \"realtime_infer\": false,\n",
      "    \"realtime_infer_request_timeout\": 60,\n",
      "    \"realtime_infer_support\": false,\n",
      "    \"train_datasets\": [\n",
      "        \"1c63db8b-5281-4aea-95dd-8f7b029039dc\"\n",
      "    ],\n",
      "    \"type\": \"vision\",\n",
      "    \"version\": \"1.0.0\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "if automl_enabled:\n",
    "    # Choose any metric that is present in the kpi dictionary present in the model's status.json. \n",
    "    # Example status.json for each model can be found in the respective section in NVIDIA TAO DOCS here: https://docs.nvidia.com/tao/tao-toolkit/text/model_zoo/cv_models/index.html\n",
    "    if model_name == \"classification_pyt\":\n",
    "        metric = \"loss\"\n",
    "    else:\n",
    "        metric = \"kpi\" \n",
    "\n",
    "    additional_automl_parameters = [] #Refer to parameter list mentioned in the above links and add any extra parameter in addition to the default enabled ones\n",
    "    remove_default_automl_parameters = [] #Remove any hyperparameters that are enabled by default for AutoML\n",
    "\n",
    "    automl_information = {\"automl_enabled\":automl_enabled,\n",
    "                          \"automl_algorithm\":automl_algorithm,\n",
    "                          \"metric\":metric,\n",
    "                          \"automl_max_recommendations\": 20, # Only for bayesian\n",
    "                          \"automl_R\": 15, # Only for hyperband\n",
    "                          \"automl_nu\": 4, # Only for hyperband\n",
    "                          \"epoch_multiplier\": 0.3, # Only for hyperband\n",
    "                          # Enable this if you want to add parameters to automl_add_hyperparameters below that are disabled by TAO in the automl_enabled column of the spec csv.\n",
    "                          # Warning: The parameters that are disabled are not tested by TAO, so there might be unexpected behaviour in overriding this\n",
    "                          \"override_automl_disabled_params\": False,\n",
    "                          \"automl_add_hyperparameters\":str(additional_automl_parameters),\n",
    "                          \"automl_remove_hyperparameters\":str(remove_default_automl_parameters)\n",
    "                         }\n",
    "    data = json.dumps(automl_information)\n",
    "\n",
    "    endpoint = f\"{base_url}/experiments/{experiment_id}\"\n",
    "\n",
    "    response = requests.patch(endpoint, data=data, headers=headers)\n",
    "    assert response.status_code in (200, 201)\n",
    "\n",
    "\n",
    "    print(response)\n",
    "    print(json.dumps(response.json(), sort_keys=True, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "id": "8df0871a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n",
      "{\n",
      "    \"data_format\": \"channels_first\",\n",
      "    \"dataset\": {\n",
      "        \"augmentation\": {\n",
      "            \"disable_horizontal_flip\": false,\n",
      "            \"enable_center_crop\": true,\n",
      "            \"enable_color_augmentation\": false,\n",
      "            \"enable_random_crop\": true,\n",
      "            \"mixup_alpha\": 0.0\n",
      "        },\n",
      "        \"image_mean\": [\n",
      "            103.939,\n",
      "            116.779,\n",
      "            123.68\n",
      "        ],\n",
      "        \"num_classes\": 20,\n",
      "        \"preprocess_mode\": \"caffe\"\n",
      "    },\n",
      "    \"gpus\": 1,\n",
      "    \"model\": {\n",
      "        \"activation_type\": \"None\",\n",
      "        \"all_projections\": false,\n",
      "        \"backbone\": \"efficientnet-b0\",\n",
      "        \"dropout\": 0.0,\n",
      "        \"freeze_bn\": false,\n",
      "        \"input_channels\": 3,\n",
      "        \"input_height\": 224,\n",
      "        \"input_image_depth\": 8,\n",
      "        \"input_width\": 224,\n",
      "        \"resize_interpolation_method\": \"bilinear\",\n",
      "        \"retain_head\": false,\n",
      "        \"use_batch_norm\": true,\n",
      "        \"use_bias\": false,\n",
      "        \"use_pooling\": true\n",
      "    },\n",
      "    \"train\": {\n",
      "        \"batch_size_per_gpu\": 64,\n",
      "        \"bn_config\": {\n",
      "            \"epsilon\": 1e-05,\n",
      "            \"momentum\": 0.9\n",
      "        },\n",
      "        \"checkpoint_interval\": 1,\n",
      "        \"clearml\": {\n",
      "            \"project\": \"TAO clearml trainings\",\n",
      "            \"tags\": [\n",
      "                \"training\",\n",
      "                \"classification\"\n",
      "            ],\n",
      "            \"task\": \"classification_train\"\n",
      "        },\n",
      "        \"label_smoothing\": 0.01,\n",
      "        \"n_workers\": 10,\n",
      "        \"num_epochs\": 80,\n",
      "        \"optim_config\": {\n",
      "            \"beta_1\": 0.99,\n",
      "            \"beta_2\": 0.99,\n",
      "            \"decay\": 0.0001,\n",
      "            \"epsilon\": 0.0001,\n",
      "            \"lr\": 0.05,\n",
      "            \"momentum\": 0.99,\n",
      "            \"nesterov\": true,\n",
      "            \"rho\": 0.5\n",
      "        },\n",
      "        \"qat\": false,\n",
      "        \"random_seed\": 42,\n",
      "        \"reg_config\": {\n",
      "            \"scope\": [\n",
      "                \"conv2d\",\n",
      "                \"dense\"\n",
      "            ],\n",
      "            \"type\": \"L2\",\n",
      "            \"weight_decay\": 5e-05\n",
      "        },\n",
      "        \"wandb\": {\n",
      "            \"entity\": \"metropolis\",\n",
      "            \"name\": \"classification_training_wandb_configdump\",\n",
      "            \"project\": \"TAO wandb trainings\",\n",
      "            \"tags\": [\n",
      "                \"training\",\n",
      "                \"classification\"\n",
      "            ]\n",
      "        }\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Get default spec schema\n",
    "endpoint = f\"{base_url}/experiments/{experiment_id}/specs/train/schema\"\n",
    "\n",
    "response = requests.get(endpoint, headers=headers)\n",
    "assert response.status_code in (200, 201)\n",
    "\n",
    "\n",
    "print(response)\n",
    "#print(response.json()) ## Uncomment for verbose schema\n",
    "train_specs = response.json()[\"default\"]\n",
    "print(json.dumps(train_specs, sort_keys=True, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "id": "3611d318",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 356,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "id": "fff8e427",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"data_format\": \"channels_first\",\n",
      "    \"dataset\": {\n",
      "        \"augmentation\": {\n",
      "            \"disable_horizontal_flip\": false,\n",
      "            \"enable_center_crop\": true,\n",
      "            \"enable_color_augmentation\": false,\n",
      "            \"enable_random_crop\": true,\n",
      "            \"mixup_alpha\": 0.0\n",
      "        },\n",
      "        \"image_mean\": [\n",
      "            103.939,\n",
      "            116.779,\n",
      "            123.68\n",
      "        ],\n",
      "        \"num_classes\": 5,\n",
      "        \"preprocess_mode\": \"caffe\"\n",
      "    },\n",
      "    \"gpus\": 1,\n",
      "    \"model\": {\n",
      "        \"activation_type\": \"None\",\n",
      "        \"all_projections\": false,\n",
      "        \"backbone\": \"efficientnet-b0\",\n",
      "        \"dropout\": 0.0,\n",
      "        \"freeze_bn\": false,\n",
      "        \"input_channels\": 3,\n",
      "        \"input_height\": 224,\n",
      "        \"input_image_depth\": 8,\n",
      "        \"input_width\": 224,\n",
      "        \"resize_interpolation_method\": \"bilinear\",\n",
      "        \"retain_head\": false,\n",
      "        \"use_batch_norm\": true,\n",
      "        \"use_bias\": false,\n",
      "        \"use_pooling\": true\n",
      "    },\n",
      "    \"train\": {\n",
      "        \"batch_size_per_gpu\": 64,\n",
      "        \"bn_config\": {\n",
      "            \"epsilon\": 1e-05,\n",
      "            \"momentum\": 0.9\n",
      "        },\n",
      "        \"checkpoint_interval\": 1,\n",
      "        \"clearml\": {\n",
      "            \"project\": \"TAO clearml trainings\",\n",
      "            \"tags\": [\n",
      "                \"training\",\n",
      "                \"classification\"\n",
      "            ],\n",
      "            \"task\": \"classification_train\"\n",
      "        },\n",
      "        \"label_smoothing\": 0.01,\n",
      "        \"n_workers\": 10,\n",
      "        \"num_epochs\": 80,\n",
      "        \"optim_config\": {\n",
      "            \"beta_1\": 0.99,\n",
      "            \"beta_2\": 0.99,\n",
      "            \"decay\": 0.0001,\n",
      "            \"epsilon\": 0.0001,\n",
      "            \"lr\": 0.05,\n",
      "            \"momentum\": 0.99,\n",
      "            \"nesterov\": true,\n",
      "            \"rho\": 0.5\n",
      "        },\n",
      "        \"qat\": false,\n",
      "        \"random_seed\": 42,\n",
      "        \"reg_config\": {\n",
      "            \"scope\": [\n",
      "                \"conv2d\",\n",
      "                \"dense\"\n",
      "            ],\n",
      "            \"type\": \"L2\",\n",
      "            \"weight_decay\": 5e-05\n",
      "        },\n",
      "        \"wandb\": {\n",
      "            \"entity\": \"metropolis\",\n",
      "            \"name\": \"classification_training_wandb_configdump\",\n",
      "            \"project\": \"TAO wandb trainings\",\n",
      "            \"tags\": [\n",
      "                \"training\",\n",
      "                \"classification\"\n",
      "            ]\n",
      "        }\n",
      "    }\n",
      "}\n",
      "Number of classes: 5\n"
     ]
    }
   ],
   "source": [
    "# Override any of the parameters listed in the previous cell as required\n",
    "# Example for multitask-classification (for each network the parameter key might be different)\n",
    "if model_name == \"multitask_classification\":\n",
    "    train_specs[\"training_config\"][\"num_epochs\"] = 5\n",
    "    train_specs[\"gpus\"] = 1\n",
    "# Example for classification_pyt\n",
    "elif model_name == \"classification_pyt\":\n",
    "    train_specs[\"train\"][\"train_config\"][\"runner\"][\"max_epochs\"] = 40\n",
    "    train_specs[\"train\"][\"num_gpus\"] = 1\n",
    "    train_specs[\"gpus\"] = 1\n",
    "# Example for classification_tf1\n",
    "elif model_name == \"classification_tf1\":\n",
    "    train_specs[\"train_config\"][\"n_epochs\"] = 80\n",
    "    train_specs[\"gpus\"] = 1\n",
    "# Example for classification_tf2\n",
    "elif model_name == \"classification_tf2\":\n",
    "    train_specs[\"train\"][\"num_epochs\"] = 80\n",
    "    train_specs[\"gpus\"] = 1\n",
    "    train_specs[\"dataset\"][\"num_classes\"] = len(class_names)\n",
    "\n",
    "print(json.dumps(train_specs, sort_keys=True, indent=4))\n",
    "print(\"Number of classes:\", train_specs[\"dataset\"][\"num_classes\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "id": "52aca403",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [201]>\n",
      "6c79c4d5-5d44-4c6b-8b76-d74c9b43610a\n",
      "{'train_classification_tf2': '6c79c4d5-5d44-4c6b-8b76-d74c9b43610a'}\n"
     ]
    }
   ],
   "source": [
    "# Run action\n",
    "parent = None\n",
    "action = \"train\"\n",
    "data = json.dumps({\"parent_job_id\":parent,\"action\":action,\"specs\":train_specs})\n",
    "\n",
    "endpoint = f\"{base_url}/experiments/{experiment_id}/jobs\"\n",
    "\n",
    "response = requests.post(endpoint, data=data, headers=headers)\n",
    "# assert response.status_code in (200, 201)\n",
    "# assert response.json()\n",
    "\n",
    "print(response)\n",
    "print(response.json())\n",
    "\n",
    "job_map[\"train_\" + model_name] = response.json()\n",
    "print(job_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "id": "56648c24",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n",
      "{\n",
      "    \"action\": \"train\",\n",
      "    \"created_on\": \"2024-08-21T06:49:50.252479\",\n",
      "    \"description\": null,\n",
      "    \"experiment_id\": \"d862a1b9-1305-4ac9-9d1f-a9e94e2b9360\",\n",
      "    \"id\": \"6c79c4d5-5d44-4c6b-8b76-d74c9b43610a\",\n",
      "    \"last_modified\": \"2024-08-21T08:30:11.913861\",\n",
      "    \"name\": null,\n",
      "    \"parent_id\": null,\n",
      "    \"result\": {\n",
      "        \"automl_result\": [\n",
      "            {\n",
      "                \"metric\": \"best_val_accuracy\",\n",
      "                \"value\": 0.662576775415039\n",
      "            }\n",
      "        ],\n",
      "        \"stats\": [\n",
      "            {\n",
      "                \"metric\": \"Estimated time for automl completion\",\n",
      "                \"value\": \"323.89 minutes remaining approximately\"\n",
      "            },\n",
      "            {\n",
      "                \"metric\": \"Current experiment number\",\n",
      "                \"value\": \"6\"\n",
      "            },\n",
      "            {\n",
      "                \"metric\": \"Number of epochs yet to start\",\n",
      "                \"value\": \"1166\"\n",
      "            },\n",
      "            {\n",
      "                \"metric\": \"Time per epoch in seconds\",\n",
      "                \"value\": \"16.67\"\n",
      "            }\n",
      "        ]\n",
      "    },\n",
      "    \"specs\": null,\n",
      "    \"status\": \"Running\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Monitor job status by repeatedly running this cell\n",
    "# For automl: Training times for different models benchmarked on 1 GPU V100 machine can be found here: https://docs.nvidia.com/tao/tao-toolkit/text/automl/automl.html#results-of-automl-experiments\n",
    "\n",
    "job_id = job_map[\"train_\" + model_name]\n",
    "endpoint = f\"{base_url}/experiments/{experiment_id}/jobs/{job_id}\"\n",
    "\n",
    "while True:    \n",
    "    clear_output(wait=True)\n",
    "    response = requests.get(endpoint, headers=headers)\n",
    "\n",
    "    if \"error_desc\" in response.json().keys() and response.json()[\"error_desc\"] in (\"Job trying to retrieve not found\", \"No AutoML run found\"):\n",
    "        print(\"Job is being created\")\n",
    "        time.sleep(5)\n",
    "        continue\n",
    "    # assert response.status_code in (200, 201)\n",
    "    print(response)\n",
    "    print(json.dumps(response.json(), sort_keys=True, indent=4))\n",
    "    assert \"status\" in response.json().keys() and response.json().get(\"status\") != \"Error\"\n",
    "    if response.json().get(\"status\") in [\"Done\",\"Error\", \"Canceled\"] or response.status_code not in (200,201):\n",
    "        break\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f692d49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## To Stop an AutoML JOB\n",
    "#    1. Stop the 'Monitor job status by repeatedly running this cell' cell (the cell right before this cell) manually\n",
    "#    2. Uncomment the snippet in the next cell and run the cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "id": "6e12845a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n",
      "{}\n"
     ]
    }
   ],
   "source": [
    "if automl_enabled:\n",
    "    job_id = job_map[\"train_\" + model_name]\n",
    "    endpoint = f\"{base_url}/experiments/{experiment_id}/jobs/{job_id}:cancel\"\n",
    "\n",
    "    response = requests.post(endpoint, headers=headers)\n",
    "    assert response.status_code in (200, 201)\n",
    "\n",
    "    print(response)\n",
    "    print(response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "be87aae2-5396-4b97-8ba9-c0a120e271e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d2d15d6b-01ea-402b-b4c6-f0d307173654\n",
      "multitask_classification\n"
     ]
    }
   ],
   "source": [
    "print(job_id)\n",
    "print(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b886030f-1541-4a47-b8f0-36db70f0f764",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e8132f13-d5da-42fd-a6fb-758d401cd1a0\n"
     ]
    }
   ],
   "source": [
    "print(experiment_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b35886aa-b63e-428d-8a75-1d79116db392",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a1c02cba-b62b-52f9-9e49-e3de0e5b66ab\n"
     ]
    }
   ],
   "source": [
    "print(user_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92364a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Resume AutoML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619ffaeb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Uncomment the below snippet if you want to resume an already stopped AutoML job and then run the 'Monitor job status by repeatedly running this cell' cell above (4th cell above from this cell)\n",
    "if automl_enabled:\n",
    "    job_id = job_map[\"train_\" + model_name]\n",
    "    endpoint = f\"{base_url}/experiments/{experiment_id}/jobs/{job_id}:resume\"\n",
    "\n",
    "    data = json.dumps({\"parent_job_id\":parent,\"specs\":train_specs})\n",
    "    response = requests.post(endpoint, data=data, headers=headers)\n",
    "    assert response.status_code in (200, 201)\n",
    "\n",
    "    print(response)\n",
    "    print(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6189a992",
   "metadata": {},
   "source": [
    "### Download train job artifacts <a class=\"anchor\" id=\"head-12\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "cbb48ef3",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    \"1e778a2d-1597-4b62-ac68-5f5ee28ddacd/automl_metadata_lock.lock\",\n",
      "    \"1e778a2d-1597-4b62-ac68-5f5ee28ddacd/controller_lock.lock\",\n",
      "    \"1e778a2d-1597-4b62-ac68-5f5ee28ddacd/recommendation_3_lock.lock\",\n",
      "    \"1e778a2d-1597-4b62-ac68-5f5ee28ddacd/recommendation_0.json\",\n",
      "    \"1e778a2d-1597-4b62-ac68-5f5ee28ddacd/recommendation_0_lock.lock\",\n",
      "    \"1e778a2d-1597-4b62-ac68-5f5ee28ddacd/recommendation_2.json\",\n",
      "    \"1e778a2d-1597-4b62-ac68-5f5ee28ddacd/experiment_2\",\n",
      "    \"1e778a2d-1597-4b62-ac68-5f5ee28ddacd/experiment_2/events\",\n",
      "    \"1e778a2d-1597-4b62-ac68-5f5ee28ddacd/experiment_2/status.json\",\n",
      "    \"1e778a2d-1597-4b62-ac68-5f5ee28ddacd/experiment_2/multitask_cls_training_log_resnet10.csv\",\n",
      "    \"1e778a2d-1597-4b62-ac68-5f5ee28ddacd/experiment_2/class_mapping.json\",\n",
      "    \"1e778a2d-1597-4b62-ac68-5f5ee28ddacd/experiment_2/weights\",\n",
      "    \"1e778a2d-1597-4b62-ac68-5f5ee28ddacd/experiment_2/log.txt\",\n",
      "    \"1e778a2d-1597-4b62-ac68-5f5ee28ddacd/experiment_0\",\n",
      "    \"1e778a2d-1597-4b62-ac68-5f5ee28ddacd/experiment_0/events\",\n",
      "    \"1e778a2d-1597-4b62-ac68-5f5ee28ddacd/experiment_0/status.json\",\n",
      "    \"1e778a2d-1597-4b62-ac68-5f5ee28ddacd/experiment_0/multitask_cls_training_log_resnet10.csv\",\n",
      "    \"1e778a2d-1597-4b62-ac68-5f5ee28ddacd/experiment_0/class_mapping.json\",\n",
      "    \"1e778a2d-1597-4b62-ac68-5f5ee28ddacd/experiment_0/weights\",\n",
      "    \"1e778a2d-1597-4b62-ac68-5f5ee28ddacd/experiment_0/log.txt\",\n",
      "    \"1e778a2d-1597-4b62-ac68-5f5ee28ddacd/recommendation_0.protobuf\",\n",
      "    \"1e778a2d-1597-4b62-ac68-5f5ee28ddacd/best_model\",\n",
      "    \"1e778a2d-1597-4b62-ac68-5f5ee28ddacd/best_model/events\",\n",
      "    \"1e778a2d-1597-4b62-ac68-5f5ee28ddacd/best_model/events/events.out.tfevents.1724132445.53ecc22a-5bcd-45e3-8072-38fae8532ebc-rwx8c\",\n",
      "    \"1e778a2d-1597-4b62-ac68-5f5ee28ddacd/best_model/status.json\",\n",
      "    \"1e778a2d-1597-4b62-ac68-5f5ee28ddacd/best_model/multitask_cls_training_log_resnet10.csv\",\n",
      "    \"1e778a2d-1597-4b62-ac68-5f5ee28ddacd/best_model/controller.json\",\n",
      "    \"1e778a2d-1597-4b62-ac68-5f5ee28ddacd/best_model/recommendation_1.protobuf\",\n",
      "    \"1e778a2d-1597-4b62-ac68-5f5ee28ddacd/best_model/class_mapping.json\",\n",
      "    \"1e778a2d-1597-4b62-ac68-5f5ee28ddacd/best_model/weights\",\n",
      "    \"1e778a2d-1597-4b62-ac68-5f5ee28ddacd/best_model/weights/multitask_cls_resnet10_epoch_012.hdf5\",\n",
      "    \"1e778a2d-1597-4b62-ac68-5f5ee28ddacd/best_model/log.txt\",\n",
      "    \"1e778a2d-1597-4b62-ac68-5f5ee28ddacd/recommendation_1_lock.lock\",\n",
      "    \"1e778a2d-1597-4b62-ac68-5f5ee28ddacd/recommendation_2_lock.lock\",\n",
      "    \"1e778a2d-1597-4b62-ac68-5f5ee28ddacd/recommendation_3.protobuf\",\n",
      "    \"1e778a2d-1597-4b62-ac68-5f5ee28ddacd/controller.json\",\n",
      "    \"1e778a2d-1597-4b62-ac68-5f5ee28ddacd/recommendation_1.protobuf\",\n",
      "    \"1e778a2d-1597-4b62-ac68-5f5ee28ddacd/current_rec.json\",\n",
      "    \"1e778a2d-1597-4b62-ac68-5f5ee28ddacd/recommendation_1.json\",\n",
      "    \"1e778a2d-1597-4b62-ac68-5f5ee28ddacd/current_rec_lock.lock\",\n",
      "    \"1e778a2d-1597-4b62-ac68-5f5ee28ddacd/brain.json\",\n",
      "    \"1e778a2d-1597-4b62-ac68-5f5ee28ddacd/automl_metadata.json\",\n",
      "    \"1e778a2d-1597-4b62-ac68-5f5ee28ddacd/controller.log\",\n",
      "    \"1e778a2d-1597-4b62-ac68-5f5ee28ddacd/experiment_3\",\n",
      "    \"1e778a2d-1597-4b62-ac68-5f5ee28ddacd/experiment_3/events\",\n",
      "    \"1e778a2d-1597-4b62-ac68-5f5ee28ddacd/experiment_3/status.json\",\n",
      "    \"1e778a2d-1597-4b62-ac68-5f5ee28ddacd/experiment_3/multitask_cls_training_log_resnet10.csv\",\n",
      "    \"1e778a2d-1597-4b62-ac68-5f5ee28ddacd/experiment_3/class_mapping.json\",\n",
      "    \"1e778a2d-1597-4b62-ac68-5f5ee28ddacd/experiment_3/weights\",\n",
      "    \"1e778a2d-1597-4b62-ac68-5f5ee28ddacd/experiment_3/log.txt\",\n",
      "    \"1e778a2d-1597-4b62-ac68-5f5ee28ddacd/recommendation_3.json\",\n",
      "    \"1e778a2d-1597-4b62-ac68-5f5ee28ddacd/brain_lock.lock\",\n",
      "    \"1e778a2d-1597-4b62-ac68-5f5ee28ddacd/recommendation_2.protobuf\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# Example to list the files of the executed train job\n",
    "job_id = job_map[\"train_\" + model_name]\n",
    "endpoint = f'{base_url}/experiments/{experiment_id}/jobs/{job_id}:list_files'\n",
    "\n",
    "response = requests.get(endpoint, headers=headers)\n",
    "print(json.dumps(response.json(), sort_keys=True, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "496f9ecf",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ## Patch the model with proper metric before training to run this cell; By default loss is used, but some models dont log the parameter under the name 'loss'\n",
    "\n",
    "# # Download selective job contents once the above job shows \"Done\" status\n",
    "# # Example to download selective files of train job (Note: will take time)\n",
    "# endpoint = f'{base_url}/experiments/{experiment_id}/jobs/{job_id}:download_selective_files'\n",
    "\n",
    "# file_lists = [] # Choose file names from the previous cell where all the files for this job were listed\n",
    "# best_model = False # Enable this to download the checkpoint of the best performing model w.r.t to the metric chosen before starting training\n",
    "# latest_model = True # Enable this to download the latest checkpoint of the training job; Disable best_model to use latest_model\n",
    "\n",
    "# params = {\"file_lists\": file_lists, \"best_model\": best_model, \"latest_model\": latest_model}\n",
    "\n",
    "# # Save\n",
    "# temptar = f'{job_id}.tar.gz'\n",
    "# with requests.get(endpoint, headers=headers, params=params, stream=True) as r:\n",
    "#     r.raise_for_status()\n",
    "#     with open(temptar, 'wb') as f:\n",
    "#         for chunk in r.iter_content(chunk_size=8192):\n",
    "#             f.write(chunk)\n",
    "\n",
    "# print(\"Untarring\")\n",
    "# # Untar to destination\n",
    "# tar_command = f'tar -xvf {temptar} -C {workdir}/'\n",
    "# os.system(tar_command)\n",
    "# os.remove(temptar)\n",
    "# print(f\"Results at {workdir}/{job_id}\")\n",
    "# model_downloaded_path = f\"{workdir}/{job_id}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "6f8e27ea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "expected_file_size:  26710944\n",
      "Requirement already satisfied: tqdm in /home/zyw/anaconda3/lib/python3.12/site-packages (4.66.4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                      | 1.02k/26.7M [00:01<9:28:40, 783B/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████▉| 26.7M/26.7M [01:30<00:00, 289kB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File size of dowloaded content until now is 26710944\n",
      "Download completed successfully.\n",
      "Untarring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 26.7M/26.7M [01:31<00:00, 293kB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results at /home/zyw/tao-env/1e778a2d-1597-4b62-ac68-5f5ee28ddacd\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Downloading train job takes a longer time, uncomment this cell if you want to still proceed\n",
    "if download_jobs:\n",
    "    endpoint = f\"{base_url}/experiments/{experiment_id}/jobs/{job_id}\"\n",
    "    response = requests.get(endpoint, headers=headers)\n",
    "    assert response.status_code in (200, 201)\n",
    "    expected_file_size = response.json().get(\"job_tar_stats\", {}).get(\"file_size\")\n",
    "    print(\"expected_file_size: \", expected_file_size)\n",
    "\n",
    "    !python3 -m pip install tqdm\n",
    "    from tqdm import tqdm\n",
    "\n",
    "    endpoint = f'{base_url}/experiments/{experiment_id}/jobs/{job_id}:download'\n",
    "    temptar = f'{job_id}.tar.gz'\n",
    "\n",
    "    with tqdm(total=expected_file_size, unit='B', unit_scale=True) as progress_bar:\n",
    "        while True:\n",
    "            # Check if the file already exists\n",
    "            headers_download_job = dict(headers)\n",
    "            if os.path.exists(temptar):\n",
    "                # Get the current file size\n",
    "                file_size = os.path.getsize(temptar)\n",
    "                print(f\"File size of dowloaded content until now is {file_size}\")\n",
    "\n",
    "                # If the file size matches the expected size, break out of the loop\n",
    "                if file_size >= (expected_file_size-1):\n",
    "                    print(\"Download completed successfully.\")\n",
    "                    print(\"Untarring\")\n",
    "                    # Untar to destination\n",
    "                    tar_command = f'tar -xf {temptar} -C {workdir}/'\n",
    "                    os.system(tar_command)\n",
    "                    os.remove(temptar)\n",
    "                    print(f\"Results at {workdir}/{job_id}\")\n",
    "                    model_downloaded_path = f\"{workdir}/{job_id}\"\n",
    "                    break\n",
    "\n",
    "                # Set the headers to resume the download from where it left off\n",
    "                headers_download_job['Range'] = f'bytes={file_size}-'\n",
    "            # Open the file for writing in binary mode\n",
    "            with open(temptar, 'ab') as f:\n",
    "                try:\n",
    "                    response = requests.get(endpoint, headers=headers_download_job, stream=True)\n",
    "                    print(response)\n",
    "                    # Check if the request was successful\n",
    "                    if response.status_code in [200, 206]:\n",
    "                        # Iterate over the content in chunks\n",
    "                        for chunk in response.iter_content(chunk_size=1024):\n",
    "                            if chunk:\n",
    "                                # Write the chunk to the file\n",
    "                                f.write(chunk)\n",
    "                                # Flush and sync the file to disk\n",
    "                                f.flush()\n",
    "                                os.fsync(f.fileno())\n",
    "                            progress_bar.update(len(chunk))\n",
    "                    else:\n",
    "                        print(f\"Failed to download file. Status code: {response.status_code}\")\n",
    "                except requests.exceptions.RequestException as e:\n",
    "                    print(\"Connection interrupted during download, resuming download from breaking point\")\n",
    "                    time.sleep(5)  # Sleep for a while before retrying the request\n",
    "                    continue  # Continue the loop to retry the request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f62fc771",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checkpoints for the training experiment\n",
      "Folder: /home/zyw/tao-env/1e778a2d-1597-4b62-ac68-5f5ee28ddacd/best_model/weights\n",
      "Files: ['multitask_cls_resnet10_epoch_012.hdf5']\n",
      "\n",
      "Results of all experiments\n",
      "   id     result\n",
      "0   0  21.493010\n",
      "1   1   2.616403\n",
      "2   2  40.618683\n",
      "3   3  28.198430\n"
     ]
    }
   ],
   "source": [
    "# View the checkpoints generated for the training job and for automl jobs, in addition view: best performing model's config and the results of all automl experiments\n",
    "\n",
    "if download_jobs:\n",
    "    if automl_enabled:\n",
    "        # !python3 -m pip install pandas==1.5.1\n",
    "        import pandas as pd\n",
    "        model_downloaded_path = f\"{model_downloaded_path}/best_model\"\n",
    "        assert glob.glob(f\"{model_downloaded_path}/*.protobuf\") or glob.glob(f\"{model_downloaded_path}/*.yaml\")\n",
    "\n",
    "    assert os.path.exists(model_downloaded_path)\n",
    "    assert (glob.glob(model_downloaded_path + \"/**/*.tlt\", recursive=True) + glob.glob(model_downloaded_path + \"/**/*.hdf5\", recursive=True) + glob.glob(model_downloaded_path + \"/**/*.pth\", recursive=True))\n",
    "\n",
    "    if os.path.exists(model_downloaded_path):        \n",
    "        #List the binary model file\n",
    "        print(\"\\nCheckpoints for the training experiment\")\n",
    "        if os.path.exists(model_downloaded_path+\"/train/weights\") and len(os.listdir(model_downloaded_path+\"/train/weights\")) > 0:\n",
    "            print(f\"Folder: {model_downloaded_path}/train/weights\")\n",
    "            print(\"Files:\", os.listdir(model_downloaded_path+\"/train/weights\"))\n",
    "        elif os.path.exists(model_downloaded_path+\"/weights\") and len(os.listdir(model_downloaded_path+\"/weights\")) > 0:\n",
    "            print(f\"Folder: {model_downloaded_path}/weights\")\n",
    "            print(\"Files:\", os.listdir(model_downloaded_path+\"/weights\"))\n",
    "        else:\n",
    "            print(f\"Folder: {model_downloaded_path}\")\n",
    "            print(\"Files:\", os.listdir(model_downloaded_path))\n",
    "\n",
    "        if automl_enabled:\n",
    "            assert glob.glob(f\"{model_downloaded_path}/*.protobuf\") or glob.glob(f\"{model_downloaded_path}/*.yaml\")\n",
    "            experiment_artifacts = json.load(open(f\"{model_downloaded_path}/controller.json\",\"r\"))\n",
    "            data_frame = pd.DataFrame(experiment_artifacts)\n",
    "            # Print experiment id/number and the corresponding result\n",
    "            print(\"\\nResults of all experiments\")\n",
    "            with pd.option_context('display.max_rows', None, 'display.max_columns', None, 'display.max_colwidth', None):\n",
    "                print(data_frame[[\"id\",\"result\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4c40c1",
   "metadata": {},
   "source": [
    "### Evaluate <a class=\"anchor\" id=\"head-12\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "922a8cfc",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'checkpoint_choose_method': 'best_model', 'checkpoint_epoch_number': {'best_model_9e9cc34b-a96e-4eab-a2b2-288e5bd941cc': 3, 'latest_model_9e9cc34b-a96e-4eab-a2b2-288e5bd941cc': 3}}\n"
     ]
    }
   ],
   "source": [
    "# Get model handler parameters\n",
    "endpoint = f\"{base_url}/experiments/{experiment_id}\"\n",
    "response = requests.get(endpoint, headers=headers)\n",
    "assert response.status_code in (200, 201)\n",
    "assert response.json()\n",
    "\n",
    "model_parameters = response.json()\n",
    "update_checkpoint_choosing = {}\n",
    "update_checkpoint_choosing[\"checkpoint_choose_method\"] = model_parameters[\"checkpoint_choose_method\"]\n",
    "update_checkpoint_choosing[\"checkpoint_epoch_number\"] = model_parameters[\"checkpoint_epoch_number\"]\n",
    "print(update_checkpoint_choosing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "060788ae",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n",
      "{\n",
      "    \"actions\": [\n",
      "        \"train\",\n",
      "        \"evaluate\",\n",
      "        \"prune\",\n",
      "        \"retrain\",\n",
      "        \"export\",\n",
      "        \"gen_trt_engine\",\n",
      "        \"inference\"\n",
      "    ],\n",
      "    \"additional_id_info\": null,\n",
      "    \"automl_R\": 15,\n",
      "    \"automl_add_hyperparameters\": \"[]\",\n",
      "    \"automl_algorithm\": \"hyperband\",\n",
      "    \"automl_enabled\": true,\n",
      "    \"automl_max_recommendations\": 20,\n",
      "    \"automl_nu\": 4,\n",
      "    \"automl_remove_hyperparameters\": \"[]\",\n",
      "    \"base_experiment\": [\n",
      "        \"cd5a8d37-132f-5811-b84c-c16ffe61b9e3\"\n",
      "    ],\n",
      "    \"calibration_dataset\": \"7998bcf9-1e40-4e3c-9b6c-675b60d24e6d\",\n",
      "    \"checkpoint_choose_method\": \"latest_model\",\n",
      "    \"checkpoint_epoch_number\": {\n",
      "        \"best_model_9e9cc34b-a96e-4eab-a2b2-288e5bd941cc\": 3,\n",
      "        \"latest_model_9e9cc34b-a96e-4eab-a2b2-288e5bd941cc\": 3\n",
      "    },\n",
      "    \"created_on\": \"2024-08-20T08:52:14.162214\",\n",
      "    \"dataset_type\": \"image_classification\",\n",
      "    \"description\": \"My Experiments\",\n",
      "    \"docker_env_vars\": {},\n",
      "    \"encryption_key\": \"nvidia_tlt\",\n",
      "    \"eval_dataset\": \"112b79fa-e93e-4487-89a1-43b5954fdf71\",\n",
      "    \"id\": \"a92eff4a-2c58-48d2-8cde-6499d3c0c939\",\n",
      "    \"inference_dataset\": \"2e94065b-c972-4002-98b9-480c09d8f98a\",\n",
      "    \"is_ptm_backbone\": true,\n",
      "    \"jobs\": [\n",
      "        {\n",
      "            \"action\": \"train\",\n",
      "            \"created_on\": \"2024-08-20T09:09:20.141847\",\n",
      "            \"description\": null,\n",
      "            \"experiment_id\": \"a92eff4a-2c58-48d2-8cde-6499d3c0c939\",\n",
      "            \"id\": \"9e9cc34b-a96e-4eab-a2b2-288e5bd941cc\",\n",
      "            \"job_tar_stats\": {\n",
      "                \"file_size\": 38442755,\n",
      "                \"sha256_digest\": \"eb80a07ec42e58e5323bc223200c197400d6b361a4f201a677348464dfc3313c\"\n",
      "            },\n",
      "            \"last_modified\": \"2024-08-20T09:27:50.690855\",\n",
      "            \"name\": null,\n",
      "            \"parent_id\": null,\n",
      "            \"result\": {\n",
      "                \"detailed_status\": {\n",
      "                    \"message\": \"\"\n",
      "                }\n",
      "            },\n",
      "            \"specs\": null,\n",
      "            \"status\": \"Done\"\n",
      "        },\n",
      "        {\n",
      "            \"action\": \"train\",\n",
      "            \"created_on\": \"2024-08-20T09:00:12.500837\",\n",
      "            \"description\": null,\n",
      "            \"experiment_id\": \"a92eff4a-2c58-48d2-8cde-6499d3c0c939\",\n",
      "            \"id\": \"6752e8bd-2930-4f15-a874-bfbf1a3218ce\",\n",
      "            \"last_modified\": \"2024-08-20T09:06:26.219474\",\n",
      "            \"name\": null,\n",
      "            \"parent_id\": null,\n",
      "            \"result\": {\n",
      "                \"detailed_status\": {\n",
      "                    \"message\": \"GPU's needed to run this job is not available yet, please wait for other jobs to complete\"\n",
      "                }\n",
      "            },\n",
      "            \"specs\": null,\n",
      "            \"status\": \"Canceled\"\n",
      "        }\n",
      "    ],\n",
      "    \"last_modified\": \"2024-08-20T09:09:14.079003\",\n",
      "    \"logo\": \"https://www.nvidia.com\",\n",
      "    \"metric\": \"kpi\",\n",
      "    \"model_params\": {},\n",
      "    \"name\": \"My Experiment\",\n",
      "    \"network_arch\": \"classification_tf2\",\n",
      "    \"ngc_path\": \"\",\n",
      "    \"public\": false,\n",
      "    \"read_only\": false,\n",
      "    \"realtime_infer\": false,\n",
      "    \"realtime_infer_request_timeout\": 60,\n",
      "    \"realtime_infer_support\": false,\n",
      "    \"train_datasets\": [\n",
      "        \"7998bcf9-1e40-4e3c-9b6c-675b60d24e6d\"\n",
      "    ],\n",
      "    \"type\": \"vision\",\n",
      "    \"version\": \"1.0.0\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Change the method by which checkpoint from the parent action is chosen, when parent action is a train/retrain action.\n",
    "# Example for evaluate action below, can be applied in the same way for other actions too\n",
    "update_checkpoint_choosing[\"checkpoint_choose_method\"] = \"latest_model\" # Choose between best_model/latest_model/from_epoch_number\n",
    "# If from_epoch_number is chosen then assign the epoch number to the dictionary key in the format 'from_epoch_number{train_job_id}'\n",
    "# update_checkpoint_choosing[\"checkpoint_epoch_number\"][\"from_epoch_number_28a2754e-50ef-43a8-9733-98913776dd90\"] = 3\n",
    "data = json.dumps(update_checkpoint_choosing)\n",
    "\n",
    "endpoint = f\"{base_url}/experiments/{experiment_id}\"\n",
    "\n",
    "response = requests.patch(endpoint, data=data, headers=headers)\n",
    "assert response.status_code in (200, 201)\n",
    "\n",
    "print(response)\n",
    "print(json.dumps(response.json(), sort_keys=True, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "09f249ff",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n",
      "{\n",
      "    \"data_format\": \"channels_first\",\n",
      "    \"dataset\": {\n",
      "        \"augmentation\": {\n",
      "            \"disable_horizontal_flip\": false,\n",
      "            \"enable_center_crop\": true,\n",
      "            \"enable_color_augmentation\": false,\n",
      "            \"enable_random_crop\": true,\n",
      "            \"mixup_alpha\": 0.0\n",
      "        },\n",
      "        \"image_mean\": [\n",
      "            103.939,\n",
      "            116.779,\n",
      "            123.68\n",
      "        ],\n",
      "        \"num_classes\": 20,\n",
      "        \"preprocess_mode\": \"caffe\"\n",
      "    },\n",
      "    \"evaluate\": {\n",
      "        \"batch_size\": 1,\n",
      "        \"n_workers\": 1,\n",
      "        \"top_k\": 3\n",
      "    },\n",
      "    \"model\": {\n",
      "        \"activation_type\": \"None\",\n",
      "        \"all_projections\": false,\n",
      "        \"backbone\": \"efficientnet-b0\",\n",
      "        \"dropout\": 0.0,\n",
      "        \"freeze_bn\": false,\n",
      "        \"input_channels\": 3,\n",
      "        \"input_height\": 224,\n",
      "        \"input_image_depth\": 8,\n",
      "        \"input_width\": 224,\n",
      "        \"resize_interpolation_method\": \"bilinear\",\n",
      "        \"retain_head\": false,\n",
      "        \"use_batch_norm\": true,\n",
      "        \"use_bias\": false,\n",
      "        \"use_pooling\": true\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Get default spec schema\n",
    "endpoint = f\"{base_url}/experiments/{experiment_id}/specs/evaluate/schema\"\n",
    "\n",
    "response = requests.get(endpoint, headers=headers)\n",
    "assert response.status_code in (200, 201)\n",
    "\n",
    "print(response)\n",
    "#print(response.json()) ## Uncomment for verbose schema\n",
    "assert \"default\" in response.json().keys()\n",
    "eval_specs = response.json()[\"default\"]\n",
    "print(json.dumps(eval_specs, sort_keys=True, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "f190dc54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data_format': 'channels_first',\n",
       " 'dataset': {'augmentation': {'disable_horizontal_flip': False,\n",
       "   'enable_center_crop': True,\n",
       "   'enable_color_augmentation': False,\n",
       "   'enable_random_crop': True,\n",
       "   'mixup_alpha': 0.0},\n",
       "  'image_mean': [103.939, 116.779, 123.68],\n",
       "  'num_classes': 20,\n",
       "  'preprocess_mode': 'caffe'},\n",
       " 'evaluate': {'batch_size': 1, 'n_workers': 1, 'top_k': 3},\n",
       " 'model': {'activation_type': 'None',\n",
       "  'all_projections': False,\n",
       "  'backbone': 'efficientnet-b0',\n",
       "  'dropout': 0.0,\n",
       "  'freeze_bn': False,\n",
       "  'input_channels': 3,\n",
       "  'input_height': 224,\n",
       "  'input_image_depth': 8,\n",
       "  'input_width': 224,\n",
       "  'resize_interpolation_method': 'bilinear',\n",
       "  'retain_head': False,\n",
       "  'use_batch_norm': True,\n",
       "  'use_bias': False,\n",
       "  'use_pooling': True}}"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_specs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "cf7c2e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_specs[\"dataset\"][\"num_classes\"] = len(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "079d1ff6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_specs[\"dataset\"][\"num_classes\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "2bdba141",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"data_format\": \"channels_first\",\n",
      "    \"dataset\": {\n",
      "        \"augmentation\": {\n",
      "            \"disable_horizontal_flip\": false,\n",
      "            \"enable_center_crop\": true,\n",
      "            \"enable_color_augmentation\": false,\n",
      "            \"enable_random_crop\": true,\n",
      "            \"mixup_alpha\": 0.0\n",
      "        },\n",
      "        \"image_mean\": [\n",
      "            103.939,\n",
      "            116.779,\n",
      "            123.68\n",
      "        ],\n",
      "        \"num_classes\": 5,\n",
      "        \"preprocess_mode\": \"caffe\"\n",
      "    },\n",
      "    \"evaluate\": {\n",
      "        \"batch_size\": 1,\n",
      "        \"n_workers\": 1,\n",
      "        \"top_k\": 3\n",
      "    },\n",
      "    \"model\": {\n",
      "        \"activation_type\": \"None\",\n",
      "        \"all_projections\": false,\n",
      "        \"backbone\": \"efficientnet-b0\",\n",
      "        \"dropout\": 0.0,\n",
      "        \"freeze_bn\": false,\n",
      "        \"input_channels\": 3,\n",
      "        \"input_height\": 224,\n",
      "        \"input_image_depth\": 8,\n",
      "        \"input_width\": 224,\n",
      "        \"resize_interpolation_method\": \"bilinear\",\n",
      "        \"retain_head\": false,\n",
      "        \"use_batch_norm\": true,\n",
      "        \"use_bias\": false,\n",
      "        \"use_pooling\": true\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Modify specs dictionary to change any config parameters\n",
    "print(json.dumps(eval_specs, sort_keys=True, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "b4e5794e",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [201]>\n",
      "a5122cd1-cd6e-45b7-a8d6-581dd7490384\n",
      "{'train_multitask_classification': '1e778a2d-1597-4b62-ac68-5f5ee28ddacd', 'evaluate_multitask_classification': 'a5122cd1-cd6e-45b7-a8d6-581dd7490384'}\n"
     ]
    }
   ],
   "source": [
    "# Run action\n",
    "parent = job_map[\"train_\" + model_name]\n",
    "action = \"evaluate\"\n",
    "data = json.dumps({\"parent_job_id\":parent,\"action\":action,\"specs\":eval_specs})\n",
    "\n",
    "endpoint = f\"{base_url}/experiments/{experiment_id}/jobs\"\n",
    "\n",
    "response = requests.post(endpoint, data=data, headers=headers)\n",
    "assert response.status_code in (200, 201)\n",
    "assert response.json()\n",
    "\n",
    "print(response)\n",
    "print(response.json())\n",
    "\n",
    "job_map[\"evaluate_\" + model_name] = response.json()\n",
    "print(job_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "3b0c315a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n",
      "{'action': 'evaluate', 'created_on': '2024-08-20T06:12:02.815184', 'description': '', 'experiment_id': 'e8132f13-d5da-42fd-a6fb-758d401cd1a0', 'id': 'a5122cd1-cd6e-45b7-a8d6-581dd7490384', 'job_tar_stats': {'file_size': 2671, 'sha256_digest': '414c9aa1fde72dae77fd05a89b5690bbc1f072a51f6a0c927f8358f4ccc4ed7f'}, 'last_modified': '2024-08-20T06:14:36.952850', 'name': '', 'parent_id': '1e778a2d-1597-4b62-ac68-5f5ee28ddacd', 'result': {'categorical': [], 'cur_iter': None, 'detailed_status': {'date': '8/20/2024', 'message': 'Evalation finished successfully.', 'status': 'SUCCESS', 'time': '6:14:12'}, 'epoch': None, 'eta': None, 'graphical': [], 'key_metric': 0.0, 'kpi': [{'metric': 'base_color', 'values': {'0': 0.6744260204081632}}, {'metric': 'category', 'values': {'0': 0.8995535714285714}}, {'metric': 'season', 'values': {'0': 0.6552933673469388}}, {'metric': 'mean accuracy', 'values': {'0': 0.743090986394558}}], 'max_epoch': None, 'time_per_epoch': None, 'time_per_iter': None}, 'specs': {'batch_size': 1, 'model_config': {'all_projections': True, 'arch': 'resnet', 'input_image_size': '3,80,60', 'n_layers': 10, 'use_batch_norm': True}, 'random_seed': 42, 'training_config': {'batch_size_per_gpu': 32, 'checkpoint_interval': 1, 'learning_rate': {'soft_start_annealing_schedule': {'annealing': 0.7, 'max_learning_rate': 0.01, 'min_learning_rate': 1e-06, 'soft_start': 0.1}}, 'num_epochs': 10, 'optimizer': {'sgd': {'momentum': 0.9, 'nesterov': False}}, 'regularizer': {'type': '__L1__', 'weight': 9e-05}}}, 'status': 'Done'}\n"
     ]
    }
   ],
   "source": [
    "# Monitor job status by repeatedly running this cell\n",
    "job_id = job_map[\"evaluate_\" + model_name]\n",
    "endpoint = f\"{base_url}/experiments/{experiment_id}/jobs/{job_id}\"\n",
    "\n",
    "while True:    \n",
    "    clear_output(wait=True)\n",
    "    response = requests.get(endpoint, headers=headers)\n",
    "    assert response.status_code in (200, 201)\n",
    "    print(response)\n",
    "    print(response.json())\n",
    "    assert \"status\" in response.json().keys() and response.json().get(\"status\") != \"Error\"\n",
    "    if response.json().get(\"status\") in [\"Done\",\"Error\", \"Canceled\"] or response.status_code not in (200,201):\n",
    "        break\n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "fe7fdcab-0784-46d1-a5d9-024b00c9eb8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'action': 'evaluate',\n",
       " 'created_on': '2024-08-20T06:12:02.815184',\n",
       " 'description': '',\n",
       " 'experiment_id': 'e8132f13-d5da-42fd-a6fb-758d401cd1a0',\n",
       " 'id': 'a5122cd1-cd6e-45b7-a8d6-581dd7490384',\n",
       " 'job_tar_stats': {'file_size': 2671,\n",
       "  'sha256_digest': '414c9aa1fde72dae77fd05a89b5690bbc1f072a51f6a0c927f8358f4ccc4ed7f'},\n",
       " 'last_modified': '2024-08-20T06:14:36.952850',\n",
       " 'name': '',\n",
       " 'parent_id': '1e778a2d-1597-4b62-ac68-5f5ee28ddacd',\n",
       " 'result': {'categorical': [],\n",
       "  'cur_iter': None,\n",
       "  'detailed_status': {'date': '8/20/2024',\n",
       "   'message': 'Evalation finished successfully.',\n",
       "   'status': 'SUCCESS',\n",
       "   'time': '6:14:12'},\n",
       "  'epoch': None,\n",
       "  'eta': None,\n",
       "  'graphical': [],\n",
       "  'key_metric': 0.0,\n",
       "  'kpi': [{'metric': 'base_color', 'values': {'0': 0.6744260204081632}},\n",
       "   {'metric': 'category', 'values': {'0': 0.8995535714285714}},\n",
       "   {'metric': 'season', 'values': {'0': 0.6552933673469388}},\n",
       "   {'metric': 'mean accuracy', 'values': {'0': 0.743090986394558}}],\n",
       "  'max_epoch': None,\n",
       "  'time_per_epoch': None,\n",
       "  'time_per_iter': None},\n",
       " 'specs': {'batch_size': 1,\n",
       "  'model_config': {'all_projections': True,\n",
       "   'arch': 'resnet',\n",
       "   'input_image_size': '3,80,60',\n",
       "   'n_layers': 10,\n",
       "   'use_batch_norm': True},\n",
       "  'random_seed': 42,\n",
       "  'training_config': {'batch_size_per_gpu': 32,\n",
       "   'checkpoint_interval': 1,\n",
       "   'learning_rate': {'soft_start_annealing_schedule': {'annealing': 0.7,\n",
       "     'max_learning_rate': 0.01,\n",
       "     'min_learning_rate': 1e-06,\n",
       "     'soft_start': 0.1}},\n",
       "   'num_epochs': 10,\n",
       "   'optimizer': {'sgd': {'momentum': 0.9, 'nesterov': False}},\n",
       "   'regularizer': {'type': '__L1__', 'weight': 9e-05}}},\n",
       " 'status': 'Done'}"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d5565c",
   "metadata": {},
   "source": [
    "### Prune, Retrain and Evaluation <a class=\"anchor\" id=\"head-13\"></a>\n",
    "\n",
    "- We optimize the trained model by pruning and retraining in the following cells"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a0d591",
   "metadata": {},
   "source": [
    "#### Prune <a class=\"anchor\" id=\"head-14\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "ad523447",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n",
      "{\n",
      "    \"equalization_criterion\": \"union\",\n",
      "    \"min_num_filters\": 16,\n",
      "    \"normalizer\": \"max\",\n",
      "    \"pruning_granularity\": 8,\n",
      "    \"pruning_threshold\": 0.1\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "if model_name != \"classification_pyt\":\n",
    "    # Get default spec schema\n",
    "    endpoint = f\"{base_url}/experiments/{experiment_id}/specs/prune/schema\"\n",
    "\n",
    "    response = requests.get(endpoint, headers=headers)\n",
    "    assert response.status_code in (200, 201)\n",
    "\n",
    "    print(response)\n",
    "    #print(response.json()) ## Uncomment for verbose schema\n",
    "    assert \"default\" in response.json().keys()\n",
    "    prune_specs = response.json()[\"default\"]\n",
    "    print(json.dumps(prune_specs, sort_keys=True, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "439a9665",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"equalization_criterion\": \"union\",\n",
      "    \"min_num_filters\": 16,\n",
      "    \"normalizer\": \"max\",\n",
      "    \"pruning_granularity\": 8,\n",
      "    \"pruning_threshold\": 0.1\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "if model_name != \"classification_pyt\":\n",
    "    # Apply changes to specs dictionary if required here\n",
    "    if model_name == \"classification_tf2\":\n",
    "        prune_specs[\"prune\"][\"byom_model_path\"] = \"\"\n",
    "    print(json.dumps(prune_specs, sort_keys=True, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "4662d1f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [201]>\n",
      "4e99ea1e-d8ce-4ff6-b8f9-d834a5c82c14\n",
      "{'train_multitask_classification': '1e778a2d-1597-4b62-ac68-5f5ee28ddacd', 'evaluate_multitask_classification': 'a5122cd1-cd6e-45b7-a8d6-581dd7490384', 'prune_multitask_classification': '4e99ea1e-d8ce-4ff6-b8f9-d834a5c82c14'}\n"
     ]
    }
   ],
   "source": [
    "if model_name != \"classification_pyt\":\n",
    "    # Run actions\n",
    "    parent = job_map[\"train_\" + model_name]\n",
    "    action = \"prune\"\n",
    "    data = json.dumps({\"parent_job_id\":parent,\"action\":action,\"specs\":prune_specs})\n",
    "\n",
    "    endpoint = f\"{base_url}/experiments/{experiment_id}/jobs\"\n",
    "\n",
    "    response = requests.post(endpoint, data=data, headers=headers)\n",
    "    assert response.status_code in (200, 201)\n",
    "    assert response.json()\n",
    "\n",
    "    print(response)\n",
    "    print(response.json())\n",
    "\n",
    "    job_map[\"prune_\" + model_name] = response.json()\n",
    "    print(job_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "bd7ef157",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n",
      "{'action': 'prune', 'created_on': '2024-08-20T06:45:05.731442', 'description': '', 'experiment_id': 'e8132f13-d5da-42fd-a6fb-758d401cd1a0', 'id': '4e99ea1e-d8ce-4ff6-b8f9-d834a5c82c14', 'job_tar_stats': {'file_size': 25665906, 'sha256_digest': 'cbff0bb261137d5cf722e814971795704d6ed364b343e2c0bce373db714d552a'}, 'last_modified': '2024-08-20T06:45:57.551625', 'name': '', 'parent_id': '1e778a2d-1597-4b62-ac68-5f5ee28ddacd', 'result': {'categorical': [], 'cur_iter': None, 'detailed_status': {'date': '8/20/2024', 'message': 'Pruning finished successfully.', 'status': 'SUCCESS', 'time': '6:45:32'}, 'epoch': None, 'eta': None, 'graphical': [], 'key_metric': 0.0, 'kpi': [{'metric': 'pruning_ratio', 'values': {'0': 1.0}}, {'metric': 'size', 'values': {'0': 26.545745849609375}}, {'metric': 'param_count', 'values': {'0': 6.910489}}], 'max_epoch': None, 'time_per_epoch': None, 'time_per_iter': None}, 'specs': {'equalization_criterion': 'union', 'min_num_filters': 16, 'normalizer': 'max', 'pruning_granularity': 8, 'pruning_threshold': 0.1}, 'status': 'Done'}\n"
     ]
    }
   ],
   "source": [
    "if model_name != \"classification_pyt\":\n",
    "    # Monitor job status by repeatedly running this cell (prune)\n",
    "    job_id = job_map[\"prune_\" + model_name]\n",
    "    endpoint = f\"{base_url}/experiments/{experiment_id}/jobs/{job_id}\"\n",
    "\n",
    "    while True:    \n",
    "        clear_output(wait=True)\n",
    "        response = requests.get(endpoint, headers=headers)\n",
    "        assert response.status_code in (200, 201)\n",
    "        print(response)\n",
    "        print(response.json())\n",
    "        assert \"status\" in response.json().keys() and response.json().get(\"status\") != \"Error\"\n",
    "        if response.json().get(\"status\") in [\"Done\",\"Error\", \"Canceled\"] or response.status_code not in (200,201):\n",
    "            break\n",
    "        time.sleep(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85829c8c",
   "metadata": {},
   "source": [
    "#### Retrain <a class=\"anchor\" id=\"head-15\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "003c57f3",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n",
      "{\n",
      "    \"gpus\": 1,\n",
      "    \"model_config\": {\n",
      "        \"all_projections\": true,\n",
      "        \"arch\": \"resnet\",\n",
      "        \"input_image_size\": \"3,80,60\",\n",
      "        \"n_layers\": 10,\n",
      "        \"use_batch_norm\": true\n",
      "    },\n",
      "    \"random_seed\": 42,\n",
      "    \"training_config\": {\n",
      "        \"batch_size_per_gpu\": 32,\n",
      "        \"checkpoint_interval\": 1,\n",
      "        \"learning_rate\": {\n",
      "            \"soft_start_annealing_schedule\": {\n",
      "                \"annealing\": 0.7,\n",
      "                \"max_learning_rate\": 0.01,\n",
      "                \"min_learning_rate\": 1e-06,\n",
      "                \"soft_start\": 0.1\n",
      "            }\n",
      "        },\n",
      "        \"num_epochs\": 10,\n",
      "        \"optimizer\": {\n",
      "            \"sgd\": {\n",
      "                \"momentum\": 0.9,\n",
      "                \"nesterov\": false\n",
      "            }\n",
      "        },\n",
      "        \"regularizer\": {\n",
      "            \"type\": \"__L1__\",\n",
      "            \"weight\": 9e-05\n",
      "        }\n",
      "    },\n",
      "    \"use_amp\": false\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "if model_name != \"classification_pyt\":\n",
    "    # Get default spec schema\n",
    "    endpoint = f\"{base_url}/experiments/{experiment_id}/specs/retrain/schema\"\n",
    "\n",
    "    response = requests.get(endpoint, headers=headers)\n",
    "    assert response.status_code in (200, 201)\n",
    "\n",
    "    print(response)\n",
    "    #print(response.json()) ## Uncomment for verbose schema\n",
    "    retrain_specs = response.json()[\"default\"]\n",
    "    print(json.dumps(retrain_specs, sort_keys=True, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "503246c7",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"gpus\": 1,\n",
      "    \"model_config\": {\n",
      "        \"all_projections\": true,\n",
      "        \"arch\": \"resnet\",\n",
      "        \"input_image_size\": \"3,80,60\",\n",
      "        \"n_layers\": 10,\n",
      "        \"use_batch_norm\": true\n",
      "    },\n",
      "    \"random_seed\": 42,\n",
      "    \"training_config\": {\n",
      "        \"batch_size_per_gpu\": 32,\n",
      "        \"checkpoint_interval\": 1,\n",
      "        \"learning_rate\": {\n",
      "            \"soft_start_annealing_schedule\": {\n",
      "                \"annealing\": 0.7,\n",
      "                \"max_learning_rate\": 0.01,\n",
      "                \"min_learning_rate\": 1e-06,\n",
      "                \"soft_start\": 0.1\n",
      "            }\n",
      "        },\n",
      "        \"num_epochs\": 5,\n",
      "        \"optimizer\": {\n",
      "            \"sgd\": {\n",
      "                \"momentum\": 0.9,\n",
      "                \"nesterov\": false\n",
      "            }\n",
      "        },\n",
      "        \"regularizer\": {\n",
      "            \"type\": \"__L1__\",\n",
      "            \"weight\": 9e-05\n",
      "        }\n",
      "    },\n",
      "    \"use_amp\": false\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "if model_name != \"classification_pyt\":\n",
    "    # Override any of the parameters listed in the previous cell as required\n",
    "    # Example for multitask-classification (for each network the parameter key might be different)\n",
    "    if model_name == \"multitask_classification\":\n",
    "        retrain_specs[\"training_config\"][\"num_epochs\"] = 5\n",
    "        retrain_specs[\"gpus\"] = 1\n",
    "    # Example for classification_tf1\n",
    "    elif model_name == \"classification_tf1\":\n",
    "        retrain_specs[\"train_config\"][\"n_epochs\"] = 80\n",
    "        retrain_specs[\"gpus\"] = 1\n",
    "    # Example for classification_tf2\n",
    "    elif model_name == \"classification_tf2\":\n",
    "        retrain_specs[\"train\"][\"num_epochs\"] = 80\n",
    "        retrain_specs[\"gpus\"] = 1\n",
    "    print(json.dumps(retrain_specs, sort_keys=True, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "a4ce02a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [201]>\n",
      "1f94b7bd-35fb-481e-87b2-f3fce170dd08\n",
      "{'train_multitask_classification': '1e778a2d-1597-4b62-ac68-5f5ee28ddacd', 'evaluate_multitask_classification': 'a5122cd1-cd6e-45b7-a8d6-581dd7490384', 'prune_multitask_classification': '4e99ea1e-d8ce-4ff6-b8f9-d834a5c82c14', 'retrain_multitask_classification': '1f94b7bd-35fb-481e-87b2-f3fce170dd08'}\n"
     ]
    }
   ],
   "source": [
    "if model_name != \"classification_pyt\":\n",
    "    # Run actions\n",
    "    parent = job_map[\"prune_\" + model_name]\n",
    "    action = \"retrain\"\n",
    "    data = json.dumps({\"parent_job_id\":parent,\"action\":action,\"specs\":retrain_specs})\n",
    "\n",
    "    endpoint = f\"{base_url}/experiments/{experiment_id}/jobs\"\n",
    "\n",
    "    response = requests.post(endpoint, data=data, headers=headers)\n",
    "    assert response.status_code in (200, 201)\n",
    "    assert response.json()\n",
    "\n",
    "    print(response)\n",
    "    print(response.json())\n",
    "\n",
    "    job_map[\"retrain_\" + model_name] = response.json()\n",
    "    print(job_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "0e46d5f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n",
      "{'action': 'retrain', 'created_on': '2024-08-20T06:46:26.541774', 'description': '', 'experiment_id': 'e8132f13-d5da-42fd-a6fb-758d401cd1a0', 'id': '1f94b7bd-35fb-481e-87b2-f3fce170dd08', 'job_tar_stats': {'file_size': 261889330, 'sha256_digest': '917c6df3610bf8e49fc3d51ee5a3a1750266f8727004d084edf298f28c8a44c9'}, 'last_modified': '2024-08-20T06:51:01.277568', 'name': '', 'parent_id': '4e99ea1e-d8ce-4ff6-b8f9-d834a5c82c14', 'result': {'categorical': [], 'cur_iter': None, 'detailed_status': {'date': '8/20/2024', 'message': 'Multi-Task classification finished successfully', 'status': 'SUCCESS', 'time': '6:50:28'}, 'epoch': 5, 'eta': '0:00:00', 'graphical': [{'metric': 'validation_loss', 'units': None, 'values': {'1': 4.347505, '2': 3.6965528, '3': 3.4004142, '4': 2.847764, '5': 2.819906}, 'x_max': 5, 'x_min': 0, 'y_max': 4.347505, 'y_min': 0.0}, {'metric': 'val_base_color_loss', 'units': None, 'values': {'1': 1.067409, '2': 1.0198623, '3': 0.95671356, '4': 0.795387, '5': 0.7946635}, 'x_max': 5, 'x_min': 0, 'y_max': 1.067409, 'y_min': 0.0}, {'metric': 'val_category_loss', 'units': None, 'values': {'1': 0.20373659, '2': 0.16155195, '3': 0.15396455, '4': 0.08037737, '5': 0.078807645}, 'x_max': 5, 'x_min': 0, 'y_max': 0.20373659, 'y_min': 0.0}, {'metric': 'val_season_loss', 'units': None, 'values': {'1': 0.76342016, '2': 0.74866974, '3': 0.7294433, '4': 0.68844795, '5': 0.67996883}, 'x_max': 5, 'x_min': 0, 'y_max': 0.76342016, 'y_min': 0.0}, {'metric': 'val_base_color_acc', 'units': None, 'values': {'1': 0.64126277, '2': 0.6801658, '3': 0.6875, '4': 0.73533165, '5': 0.7468112}, 'x_max': 5, 'x_min': 0, 'y_max': 0.7468112, 'y_min': 0.0}, {'metric': 'val_category_acc', 'units': None, 'values': {'1': 0.93590564, '2': 0.95376277, '3': 0.9559949, '4': 0.9783163, '5': 0.9770408}, 'x_max': 5, 'x_min': 0, 'y_max': 0.9783163, 'y_min': 0.0}, {'metric': 'val_season_acc', 'units': None, 'values': {'1': 0.67251277, '2': 0.67633927, '3': 0.6817602, '4': 0.7085459, '5': 0.70376277}, 'x_max': 5, 'x_min': 0, 'y_max': 0.7085459, 'y_min': 0.0}, {'metric': 'loss', 'units': None, 'values': {'1': 4.730805, '2': 4.0187464, '3': 3.658966, '4': 3.2343085, '5': 2.801142}, 'x_max': 5, 'x_min': 0, 'y_max': 4.730805, 'y_min': 0.0}, {'metric': 'base_color_loss', 'units': None, 'values': {'1': 0.92231864, '2': 1.0178189, '3': 0.9848933, '4': 0.9054443, '5': 0.7569411}, 'x_max': 5, 'x_min': 0, 'y_max': 1.0178189, 'y_min': 0.0}, {'metric': 'category_loss', 'units': None, 'values': {'1': 0.18891908, '2': 0.21580023, '3': 0.20387773, '4': 0.15633838, '5': 0.08660813}, 'x_max': 5, 'x_min': 0, 'y_max': 0.21580023, 'y_min': 0.0}, {'metric': 'season_loss', 'units': None, 'values': {'1': 0.7369251, '2': 0.79187196, '3': 0.7788905, '4': 0.7488435, '5': 0.6882432}, 'x_max': 5, 'x_min': 0, 'y_max': 0.79187196, 'y_min': 0.0}, {'metric': 'base_color_acc', 'units': None, 'values': {'1': 0.6961581, '2': 0.67399967, '3': 0.68503904, '4': 0.70895106, '5': 0.7536665}, 'x_max': 5, 'x_min': 0, 'y_max': 0.7536665, 'y_min': 0.0}, {'metric': 'category_acc', 'units': None, 'values': {'1': 0.94293004, '2': 0.93384343, '3': 0.9395425, '4': 0.9534513, '5': 0.9737765}, 'x_max': 5, 'x_min': 0, 'y_max': 0.9737765, 'y_min': 0.0}, {'metric': 'season_acc', 'units': None, 'values': {'1': 0.6793799, '2': 0.65307665, '3': 0.6611669, '4': 0.67531484, '5': 0.7058425}, 'x_max': 5, 'x_min': 0, 'y_max': 0.7058425, 'y_min': 0.0}, {'metric': 'learning_rate', 'units': None, 'values': {'1': 0.01, '2': 0.01, '3': 0.01, '4': 0.00046415877, '5': 1e-06}, 'x_max': 5, 'x_min': 0, 'y_max': 0.01, 'y_min': 0.0}], 'key_metric': 0.0, 'kpi': [{'metric': 'val_base_color_acc', 'values': {'1': 0.64126277, '2': 0.6801658, '3': 0.6875, '4': 0.73533165, '5': 0.7468112}}, {'metric': 'val_category_acc', 'values': {'1': 0.93590564, '2': 0.95376277, '3': 0.9559949, '4': 0.9783163, '5': 0.9770408}}, {'metric': 'val_season_acc', 'values': {'1': 0.67251277, '2': 0.67633927, '3': 0.6817602, '4': 0.7085459, '5': 0.70376277}}, {'metric': 'base_color_acc', 'values': {'1': 0.6961581, '2': 0.67399967, '3': 0.68503904, '4': 0.70895106, '5': 0.7536665}}, {'metric': 'category_acc', 'values': {'1': 0.94293004, '2': 0.93384343, '3': 0.9395425, '4': 0.9534513, '5': 0.9737765}}, {'metric': 'season_acc', 'values': {'1': 0.6793799, '2': 0.65307665, '3': 0.6611669, '4': 0.67531484, '5': 0.7058425}}, {'metric': 'base_color', 'values': {'5': 0.75}}, {'metric': 'category', 'values': {'5': 0.9792729616165161}}, {'metric': 'season', 'values': {'5': 0.7082270383834839}}, {'metric': 'mean accuracy', 'values': {'5': 0.8125}}], 'max_epoch': 5, 'time_per_epoch': '0:00:29.614799', 'time_per_iter': None}, 'specs': {'gpus': 1, 'model_config': {'all_projections': True, 'arch': 'resnet', 'input_image_size': '3,80,60', 'n_layers': 10, 'use_batch_norm': True}, 'random_seed': 42, 'training_config': {'batch_size_per_gpu': 32, 'checkpoint_interval': 1, 'learning_rate': {'soft_start_annealing_schedule': {'annealing': 0.7, 'max_learning_rate': 0.01, 'min_learning_rate': 1e-06, 'soft_start': 0.1}}, 'num_epochs': 5, 'optimizer': {'sgd': {'momentum': 0.9, 'nesterov': False}}, 'regularizer': {'type': '__L1__', 'weight': 9e-05}}, 'use_amp': False}, 'status': 'Done'}\n"
     ]
    }
   ],
   "source": [
    "if model_name != \"classification_pyt\":\n",
    "    # Monitor job status by repeatedly running this cell (retrain)\n",
    "    job_id = job_map[\"retrain_\" + model_name]\n",
    "    endpoint = f\"{base_url}/experiments/{experiment_id}/jobs/{job_id}\"\n",
    "\n",
    "    while True:    \n",
    "        clear_output(wait=True)\n",
    "        response = requests.get(endpoint, headers=headers)\n",
    "        assert response.status_code in (200, 201)\n",
    "        print(response)\n",
    "        print(response.json())\n",
    "        assert \"status\" in response.json().keys() and response.json().get(\"status\") != \"Error\"\n",
    "        if response.json().get(\"status\") in [\"Done\",\"Error\", \"Canceled\"] or response.status_code not in (200,201):\n",
    "            break\n",
    "        time.sleep(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "1c3e0fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if model_name != \"classification_pyt\":\n",
    "#    # Optional cancel job - for jobs that are pending/running (retrain)\n",
    "\n",
    "#     job_id = job_map[\"retrain_\" + model_name]\n",
    "#     endpoint = f\"{base_url}/experiments/{experiment_id}/jobs/{job_id}:cancel\"\n",
    "\n",
    "#     response = requests.post(endpoint, headers=headers)\n",
    "#     assert response.status_code in (200, 201)\n",
    "\n",
    "#     print(response)\n",
    "#     print(response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9752493",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if model_name != \"classification_pyt\":\n",
    "    # # Optional delete job - for jobs that are error/done (retrain)\n",
    "\n",
    "    # job_id = job_map[\"retrain_\" + model_name]\n",
    "    # endpoint = f\"{base_url}/experiments/{experiment_id}/jobs/{job_id}\"\n",
    "\n",
    "    # response = requests.delete(endpoint, headers=headers)\n",
    "    # assert response.status_code in (200, 201)\n",
    "\n",
    "    # print(response)\n",
    "    # print(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b41929a",
   "metadata": {},
   "source": [
    "#### Evaluate after retrain <a class=\"anchor\" id=\"head-15\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "f05af1cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n",
      "{\n",
      "    \"batch_size\": 1,\n",
      "    \"model_config\": {\n",
      "        \"all_projections\": true,\n",
      "        \"arch\": \"resnet\",\n",
      "        \"input_image_size\": \"3,80,60\",\n",
      "        \"n_layers\": 10,\n",
      "        \"use_batch_norm\": true\n",
      "    },\n",
      "    \"random_seed\": 42,\n",
      "    \"training_config\": {\n",
      "        \"batch_size_per_gpu\": 32,\n",
      "        \"checkpoint_interval\": 1,\n",
      "        \"learning_rate\": {\n",
      "            \"soft_start_annealing_schedule\": {\n",
      "                \"annealing\": 0.7,\n",
      "                \"max_learning_rate\": 0.01,\n",
      "                \"min_learning_rate\": 1e-06,\n",
      "                \"soft_start\": 0.1\n",
      "            }\n",
      "        },\n",
      "        \"num_epochs\": 10,\n",
      "        \"optimizer\": {\n",
      "            \"sgd\": {\n",
      "                \"momentum\": 0.9,\n",
      "                \"nesterov\": false\n",
      "            }\n",
      "        },\n",
      "        \"regularizer\": {\n",
      "            \"type\": \"__L1__\",\n",
      "            \"weight\": 9e-05\n",
      "        }\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Get default spec schema\n",
    "if model_name != \"classification_pyt\":\n",
    "    endpoint = f\"{base_url}/experiments/{experiment_id}/specs/evaluate/schema\"\n",
    "\n",
    "    response = requests.get(endpoint, headers=headers)\n",
    "    assert response.status_code in (200, 201)\n",
    "\n",
    "    print(response)\n",
    "    #print(response.json()) ## Uncomment for verbose schema\n",
    "    assert \"default\" in response.json().keys()\n",
    "    eval_retrain_specs = response.json()[\"default\"]\n",
    "    print(json.dumps(eval_retrain_specs, sort_keys=True, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "0fad76c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"batch_size\": 1,\n",
      "    \"model_config\": {\n",
      "        \"all_projections\": true,\n",
      "        \"arch\": \"resnet\",\n",
      "        \"input_image_size\": \"3,80,60\",\n",
      "        \"n_layers\": 10,\n",
      "        \"use_batch_norm\": true\n",
      "    },\n",
      "    \"random_seed\": 42,\n",
      "    \"training_config\": {\n",
      "        \"batch_size_per_gpu\": 32,\n",
      "        \"checkpoint_interval\": 1,\n",
      "        \"learning_rate\": {\n",
      "            \"soft_start_annealing_schedule\": {\n",
      "                \"annealing\": 0.7,\n",
      "                \"max_learning_rate\": 0.01,\n",
      "                \"min_learning_rate\": 1e-06,\n",
      "                \"soft_start\": 0.1\n",
      "            }\n",
      "        },\n",
      "        \"num_epochs\": 10,\n",
      "        \"optimizer\": {\n",
      "            \"sgd\": {\n",
      "                \"momentum\": 0.9,\n",
      "                \"nesterov\": false\n",
      "            }\n",
      "        },\n",
      "        \"regularizer\": {\n",
      "            \"type\": \"__L1__\",\n",
      "            \"weight\": 9e-05\n",
      "        }\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Modify specs dictionary to change any config parameters\n",
    "if model_name != \"classification_pyt\":\n",
    "    print(json.dumps(eval_retrain_specs, sort_keys=True, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "410b062d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [201]>\n",
      "03c5383e-1b9d-4511-866a-d403eec83c5d\n",
      "{'train_multitask_classification': '1e778a2d-1597-4b62-ac68-5f5ee28ddacd', 'evaluate_multitask_classification': 'a5122cd1-cd6e-45b7-a8d6-581dd7490384', 'prune_multitask_classification': '4e99ea1e-d8ce-4ff6-b8f9-d834a5c82c14', 'retrain_multitask_classification': '1f94b7bd-35fb-481e-87b2-f3fce170dd08', 'eval_retrain_multitask_classification': '03c5383e-1b9d-4511-866a-d403eec83c5d'}\n"
     ]
    }
   ],
   "source": [
    "if model_name != \"classification_pyt\":\n",
    "    # Run actions\n",
    "    parent = job_map[\"retrain_\" + model_name]\n",
    "    action = \"evaluate\"\n",
    "    data = json.dumps({\"parent_job_id\":parent,\"action\":action,\"specs\":eval_retrain_specs})\n",
    "\n",
    "    endpoint = f\"{base_url}/experiments/{experiment_id}/jobs\"\n",
    "\n",
    "    response = requests.post(endpoint, data=data, headers=headers)\n",
    "    assert response.status_code in (200, 201)\n",
    "    assert response.json()\n",
    "\n",
    "    print(response)\n",
    "    print(response.json())\n",
    "\n",
    "    job_map[\"eval_retrain_\" + model_name] = response.json()\n",
    "    print(job_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "bf324530",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n",
      "{'action': 'evaluate', 'created_on': '2024-08-20T06:57:28.233371', 'description': '', 'experiment_id': 'e8132f13-d5da-42fd-a6fb-758d401cd1a0', 'id': '03c5383e-1b9d-4511-866a-d403eec83c5d', 'job_tar_stats': {'file_size': 2705, 'sha256_digest': '01169a9d9030b99a2bd23feeca95a875a94161e7fca44e45b9f17ca30615e2e0'}, 'last_modified': '2024-08-20T06:58:11.026204', 'name': '', 'parent_id': '1f94b7bd-35fb-481e-87b2-f3fce170dd08', 'result': {'categorical': [], 'cur_iter': None, 'detailed_status': {'date': '8/20/2024', 'message': 'Evalation finished successfully.', 'status': 'SUCCESS', 'time': '6:57:46'}, 'epoch': None, 'eta': None, 'graphical': [], 'key_metric': 0.0, 'kpi': [{'metric': 'base_color', 'values': {'0': 0.765625}}, {'metric': 'category', 'values': {'0': 0.9811862244897959}}, {'metric': 'season', 'values': {'0': 0.7146045918367347}}, {'metric': 'mean accuracy', 'values': {'0': 0.8204719387755102}}], 'max_epoch': None, 'time_per_epoch': None, 'time_per_iter': None}, 'specs': {'batch_size': 1, 'model_config': {'all_projections': True, 'arch': 'resnet', 'input_image_size': '3,80,60', 'n_layers': 10, 'use_batch_norm': True}, 'random_seed': 42, 'training_config': {'batch_size_per_gpu': 32, 'checkpoint_interval': 1, 'learning_rate': {'soft_start_annealing_schedule': {'annealing': 0.7, 'max_learning_rate': 0.01, 'min_learning_rate': 1e-06, 'soft_start': 0.1}}, 'num_epochs': 10, 'optimizer': {'sgd': {'momentum': 0.9, 'nesterov': False}}, 'regularizer': {'type': '__L1__', 'weight': 9e-05}}}, 'status': 'Done'}\n"
     ]
    }
   ],
   "source": [
    "if model_name != \"classification_pyt\":\n",
    "    # Monitor job status by repeatedly running this cell (evaluate)\n",
    "    job_id = job_map[\"eval_retrain_\" + model_name]\n",
    "    endpoint = f\"{base_url}/experiments/{experiment_id}/jobs/{job_id}\"\n",
    "\n",
    "    while True:    \n",
    "        clear_output(wait=True)\n",
    "        response = requests.get(endpoint, headers=headers)\n",
    "        assert response.status_code in (200, 201)\n",
    "        print(response)\n",
    "        print(response.json())\n",
    "        assert \"status\" in response.json().keys() and response.json().get(\"status\") != \"Error\"\n",
    "        if response.json().get(\"status\") in [\"Done\",\"Error\", \"Canceled\"] or response.status_code not in (200,201):\n",
    "            break\n",
    "        time.sleep(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "0a7f493e-0afd-43f0-85f4-595e036877d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'action': 'evaluate',\n",
       " 'created_on': '2024-08-20T06:57:28.233371',\n",
       " 'description': '',\n",
       " 'experiment_id': 'e8132f13-d5da-42fd-a6fb-758d401cd1a0',\n",
       " 'id': '03c5383e-1b9d-4511-866a-d403eec83c5d',\n",
       " 'job_tar_stats': {'file_size': 2705,\n",
       "  'sha256_digest': '01169a9d9030b99a2bd23feeca95a875a94161e7fca44e45b9f17ca30615e2e0'},\n",
       " 'last_modified': '2024-08-20T06:58:11.026204',\n",
       " 'name': '',\n",
       " 'parent_id': '1f94b7bd-35fb-481e-87b2-f3fce170dd08',\n",
       " 'result': {'categorical': [],\n",
       "  'cur_iter': None,\n",
       "  'detailed_status': {'date': '8/20/2024',\n",
       "   'message': 'Evalation finished successfully.',\n",
       "   'status': 'SUCCESS',\n",
       "   'time': '6:57:46'},\n",
       "  'epoch': None,\n",
       "  'eta': None,\n",
       "  'graphical': [],\n",
       "  'key_metric': 0.0,\n",
       "  'kpi': [{'metric': 'base_color', 'values': {'0': 0.765625}},\n",
       "   {'metric': 'category', 'values': {'0': 0.9811862244897959}},\n",
       "   {'metric': 'season', 'values': {'0': 0.7146045918367347}},\n",
       "   {'metric': 'mean accuracy', 'values': {'0': 0.8204719387755102}}],\n",
       "  'max_epoch': None,\n",
       "  'time_per_epoch': None,\n",
       "  'time_per_iter': None},\n",
       " 'specs': {'batch_size': 1,\n",
       "  'model_config': {'all_projections': True,\n",
       "   'arch': 'resnet',\n",
       "   'input_image_size': '3,80,60',\n",
       "   'n_layers': 10,\n",
       "   'use_batch_norm': True},\n",
       "  'random_seed': 42,\n",
       "  'training_config': {'batch_size_per_gpu': 32,\n",
       "   'checkpoint_interval': 1,\n",
       "   'learning_rate': {'soft_start_annealing_schedule': {'annealing': 0.7,\n",
       "     'max_learning_rate': 0.01,\n",
       "     'min_learning_rate': 1e-06,\n",
       "     'soft_start': 0.1}},\n",
       "   'num_epochs': 10,\n",
       "   'optimizer': {'sgd': {'momentum': 0.9, 'nesterov': False}},\n",
       "   'regularizer': {'type': '__L1__', 'weight': 9e-05}}},\n",
       " 'status': 'Done'}"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "032a9608",
   "metadata": {},
   "source": [
    "### Export <a class=\"anchor\" id=\"head-17\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "9b506b32",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n",
      "{\n",
      "    \"backend\": \"onnx\",\n",
      "    \"batch_size\": 4,\n",
      "    \"batches\": 10,\n",
      "    \"data_type\": \"fp32\",\n",
      "    \"force_ptq\": false,\n",
      "    \"gen_ds_config\": true,\n",
      "    \"strict_type_constraints\": false,\n",
      "    \"version\": \"1\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Get default spec schema\n",
    "endpoint = f\"{base_url}/experiments/{experiment_id}/specs/export/schema\"\n",
    "\n",
    "response = requests.get(endpoint, headers=headers)\n",
    "assert response.status_code in (200, 201)\n",
    "\n",
    "print(response)\n",
    "#print(response.json()) ## Uncomment for verbose schema\n",
    "assert \"default\" in response.json().keys()\n",
    "export_specs = response.json()[\"default\"]\n",
    "print(json.dumps(export_specs, sort_keys=True, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "6f09edca",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"backend\": \"onnx\",\n",
      "    \"batch_size\": 4,\n",
      "    \"batches\": 10,\n",
      "    \"data_type\": \"fp32\",\n",
      "    \"force_ptq\": false,\n",
      "    \"gen_ds_config\": true,\n",
      "    \"strict_type_constraints\": false,\n",
      "    \"version\": \"1\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Apply changes to spec dictionary if required\n",
    "print(json.dumps(export_specs, sort_keys=True, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "d787f74e",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [201]>\n",
      "1bc50b81-05c5-4000-b372-5e3a387d0f28\n",
      "{'train_multitask_classification': '1e778a2d-1597-4b62-ac68-5f5ee28ddacd', 'evaluate_multitask_classification': 'a5122cd1-cd6e-45b7-a8d6-581dd7490384', 'prune_multitask_classification': '4e99ea1e-d8ce-4ff6-b8f9-d834a5c82c14', 'retrain_multitask_classification': '1f94b7bd-35fb-481e-87b2-f3fce170dd08', 'eval_retrain_multitask_classification': '03c5383e-1b9d-4511-866a-d403eec83c5d', 'export_multitask_classification': '1bc50b81-05c5-4000-b372-5e3a387d0f28'}\n"
     ]
    }
   ],
   "source": [
    "# Run action\n",
    "parent = job_map[\"train_\" + model_name]\n",
    "action = \"export\"\n",
    "data = json.dumps({\"parent_job_id\":parent,\"action\":action,\"specs\":export_specs})\n",
    "\n",
    "endpoint = f\"{base_url}/experiments/{experiment_id}/jobs\"\n",
    "\n",
    "response = requests.post(endpoint, data=data, headers=headers)\n",
    "assert response.status_code in (200, 201)\n",
    "assert response.json()\n",
    "\n",
    "print(response)\n",
    "print(response.json())\n",
    "\n",
    "job_map[\"export_\" + model_name] = response.json()\n",
    "print(job_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8cf188",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n",
      "{'action': 'export', 'created_on': '2024-08-20T07:05:37.785520', 'description': '', 'experiment_id': 'e8132f13-d5da-42fd-a6fb-758d401cd1a0', 'id': '1bc50b81-05c5-4000-b372-5e3a387d0f28', 'last_modified': '2024-08-20T07:05:47.585734', 'name': '', 'parent_id': '1e778a2d-1597-4b62-ac68-5f5ee28ddacd', 'result': {'categorical': [], 'cur_iter': None, 'detailed_status': {'date': '', 'message': '', 'status': '', 'time': ''}, 'epoch': None, 'eta': None, 'graphical': [], 'key_metric': 0.0, 'kpi': [], 'max_epoch': None, 'time_per_epoch': None, 'time_per_iter': None}, 'specs': {'backend': 'onnx', 'batch_size': 4, 'batches': 10, 'data_type': 'fp32', 'force_ptq': False, 'gen_ds_config': True, 'strict_type_constraints': False, 'version': '1'}, 'status': 'Running'}\n"
     ]
    }
   ],
   "source": [
    "# Monitor job status by repeatedly running this cell\n",
    "job_id = job_map[\"export_\" + model_name]\n",
    "endpoint = f\"{base_url}/experiments/{experiment_id}/jobs/{job_id}\"\n",
    "\n",
    "while True:\n",
    "    clear_output(wait=True)\n",
    "    response = requests.get(endpoint, headers=headers)\n",
    "    assert response.status_code in (200, 201)\n",
    "    print(response)\n",
    "    print(response.json())\n",
    "    assert \"status\" in response.json().keys() and response.json().get(\"status\") != \"Error\"\n",
    "    if response.json().get(\"status\") in [\"Done\",\"Error\", \"Canceled\"] or response.status_code not in (200,201):\n",
    "        break\n",
    "    time.sleep(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81a3445",
   "metadata": {},
   "source": [
    "### TRT Engine generation using TAO-Deploy <a class=\"anchor\" id=\"head-19\"></a>\n",
    "\n",
    "- Here, we use the exported model to generate trt engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9b6d31",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get default spec schema\n",
    "endpoint = f\"{base_url}/experiments/{experiment_id}/specs/gen_trt_engine/schema\"\n",
    "\n",
    "response = requests.get(endpoint, headers=headers)\n",
    "assert response.status_code in (200, 201)\n",
    "\n",
    "print(response)\n",
    "#print(response.json()) ## Uncomment for verbose schema\n",
    "assert \"default\" in response.json().keys()\n",
    "tao_deploy_specs = response.json()[\"default\"]\n",
    "print(json.dumps(tao_deploy_specs, sort_keys=True, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf2b9f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Apply changes\n",
    "if model_name == \"classification_tf2\":\n",
    "    tao_deploy_specs[\"gen_trt_engine\"][\"tensorrt\"][\"data_type\"] = \"int8\"\n",
    "elif model_name == \"classification_pyt\":\n",
    "    tao_deploy_specs[\"gen_trt_engine\"][\"tensorrt\"][\"data_type\"] = \"fp16\"\n",
    "else:\n",
    "    tao_deploy_specs[\"data_type\"] = \"int8\"\n",
    "print(json.dumps(tao_deploy_specs, sort_keys=True, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74e5b50",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run action\n",
    "parent = job_map[\"export_\" + model_name]\n",
    "action = \"gen_trt_engine\"\n",
    "data = json.dumps({\"parent_job_id\":parent,\"action\":action,\"specs\":tao_deploy_specs})\n",
    "\n",
    "endpoint = f\"{base_url}/experiments/{experiment_id}/jobs\"\n",
    "\n",
    "response = requests.post(endpoint, data=data, headers=headers)\n",
    "assert response.status_code in (200, 201)\n",
    "assert response.json()\n",
    "\n",
    "print(response)\n",
    "print(response.json())\n",
    "\n",
    "job_map[\"model_gen_trt_engine_\" + model_name] = response.json()\n",
    "print(job_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4909bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Monitor job status by repeatedly running this cell\n",
    "job_id = job_map[\"model_gen_trt_engine_\" + model_name]\n",
    "endpoint = f\"{base_url}/experiments/{experiment_id}/jobs/{job_id}\"\n",
    "\n",
    "while True:\n",
    "    clear_output(wait=True)\n",
    "    response = requests.get(endpoint, headers=headers)\n",
    "    assert response.status_code in (200, 201)\n",
    "    print(response)\n",
    "    print(response.json())\n",
    "    assert \"status\" in response.json().keys() and response.json().get(\"status\") != \"Error\"\n",
    "    if response.json().get(\"status\") in [\"Done\",\"Error\", \"Canceled\"] or response.status_code not in (200,201):\n",
    "        break\n",
    "    time.sleep(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2f9bb6",
   "metadata": {},
   "source": [
    "### TAO inference <a class=\"anchor\" id=\"head-20\"></a>\n",
    "\n",
    "- Run inference on a set of images using the .tlt model created at train step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44574e9b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get default spec schema\n",
    "endpoint = f\"{base_url}/experiments/{experiment_id}/specs/inference/schema\"\n",
    "\n",
    "response = requests.get(endpoint, headers=headers)\n",
    "assert response.status_code in (200, 201)\n",
    "\n",
    "print(response)\n",
    "#print(response.json()) ## Uncomment for verbose schema\n",
    "assert \"default\" in response.json().keys()\n",
    "tao_inference_specs = response.json()[\"default\"]\n",
    "print(json.dumps(tao_inference_specs, sort_keys=True, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa7aba2",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Apply changes to the specs you want to modify\n",
    "print(json.dumps(tao_inference_specs, sort_keys=True, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6963d36",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Run action\n",
    "parent = job_map[\"train_\" + model_name]\n",
    "action = \"inference\"\n",
    "data = json.dumps({\"parent_job_id\":parent,\"action\":action,\"specs\":tao_inference_specs})\n",
    "\n",
    "endpoint = f\"{base_url}/experiments/{experiment_id}/jobs\"\n",
    "\n",
    "response = requests.post(endpoint, data=data, headers=headers)\n",
    "assert response.status_code in (200, 201)\n",
    "assert response.json()\n",
    "\n",
    "print(response)\n",
    "print(response.json())\n",
    "\n",
    "job_map[\"inference_tlt_\" + model_name] = response.json()\n",
    "print(job_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea64f3b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Monitor job status by repeatedly running this cell\n",
    "job_id = job_map[\"inference_tlt_\" + model_name]\n",
    "endpoint = f\"{base_url}/experiments/{experiment_id}/jobs/{job_id}\"\n",
    "\n",
    "while True:\n",
    "    clear_output(wait=True)\n",
    "    response = requests.get(endpoint, headers=headers)\n",
    "    assert response.status_code in (200, 201)\n",
    "    print(response)\n",
    "    print(response.json())\n",
    "    assert \"status\" in response.json().keys() and response.json().get(\"status\") != \"Error\"\n",
    "    if response.json().get(\"status\") in [\"Done\",\"Error\", \"Canceled\"] or response.status_code not in (200,201):\n",
    "        break\n",
    "    time.sleep(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a478de1",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Download job contents once the above job shows \"Done\" status\n",
    "if download_jobs:\n",
    "    job_id = job_map[\"inference_tlt_\" + model_name]\n",
    "    endpoint = f\"{base_url}/experiments/{experiment_id}/jobs/{job_id}\"\n",
    "    response = requests.get(endpoint, headers=headers)\n",
    "    assert response.status_code in (200, 201)\n",
    "    expected_file_size = response.json().get(\"job_tar_stats\", {}).get(\"file_size\")\n",
    "    print(\"expected_file_size: \", expected_file_size)\n",
    "\n",
    "    !python3 -m pip install tqdm\n",
    "    from tqdm import tqdm\n",
    "\n",
    "    endpoint = f'{base_url}/experiments/{experiment_id}/jobs/{job_id}:download'\n",
    "    temptar = f'{job_id}.tar.gz'\n",
    "\n",
    "    with tqdm(total=expected_file_size, unit='B', unit_scale=True) as progress_bar:\n",
    "        while True:\n",
    "            # Check if the file already exists\n",
    "            headers_download_job = dict(headers)\n",
    "            if os.path.exists(temptar):\n",
    "                # Get the current file size\n",
    "                file_size = os.path.getsize(temptar)\n",
    "                print(f\"File size of dowloaded content until now is {file_size}\")\n",
    "\n",
    "                # If the file size matches the expected size, break out of the loop\n",
    "                if file_size >= (expected_file_size-1):\n",
    "                    print(\"Download completed successfully.\")\n",
    "                    print(\"Untarring\")\n",
    "                    # Untar to destination\n",
    "                    tar_command = f'tar -xf {temptar} -C {workdir}/'\n",
    "                    os.system(tar_command)\n",
    "                    os.remove(temptar)\n",
    "                    print(f\"Results at {workdir}/{job_id}\")\n",
    "                    inference_out_path = f\"{workdir}/{job_id}\"\n",
    "                    break\n",
    "\n",
    "                # Set the headers to resume the download from where it left off\n",
    "                headers_download_job['Range'] = f'bytes={file_size}-'\n",
    "            # Open the file for writing in binary mode\n",
    "            with open(temptar, 'ab') as f:\n",
    "                try:\n",
    "                    response = requests.get(endpoint, headers=headers_download_job, stream=True)\n",
    "                    print(response)\n",
    "                    # Check if the request was successful\n",
    "                    if response.status_code in [200, 206]:\n",
    "                        # Iterate over the content in chunks\n",
    "                        for chunk in response.iter_content(chunk_size=1024):\n",
    "                            if chunk:\n",
    "                                # Write the chunk to the file\n",
    "                                f.write(chunk)\n",
    "                                # Flush and sync the file to disk\n",
    "                                f.flush()\n",
    "                                os.fsync(f.fileno())\n",
    "                            progress_bar.update(len(chunk))\n",
    "                    else:\n",
    "                        print(f\"Failed to download file. Status code: {response.status_code}\")\n",
    "                except requests.exceptions.RequestException as e:\n",
    "                    print(\"Connection interrupted during download, resuming download from breaking point\")\n",
    "                    time.sleep(5)  # Sleep for a while before retrying the request\n",
    "                    continue  # Continue the loop to retry the request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05579e86",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Print Classification results\n",
    "if download_jobs:\n",
    "    if model_name == \"classification_tf1\":\n",
    "        assert os.path.exists(f'{inference_out_path}/result.csv')\n",
    "        !cat {inference_out_path}/result.csv\n",
    "    elif \"classification_\" in model_name:\n",
    "        assert os.path.exists(f'{inference_out_path}/inference/result.csv')\n",
    "        !cat {inference_out_path}/inference/result.csv\n",
    "    elif model_name == \"multitask_classification\":\n",
    "        assert os.path.exists(f'{inference_out_path}/result.txt')\n",
    "        !cat {inference_out_path}/result.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c82226",
   "metadata": {},
   "source": [
    "### TRT inference <a class=\"anchor\" id=\"head-21\"></a>\n",
    "\n",
    "- no need to change the specs since we already uploaded it at the tlt inference step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef286b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get default spec schema\n",
    "endpoint = f\"{base_url}/experiments/{experiment_id}/specs/inference/schema\"\n",
    "\n",
    "response = requests.get(endpoint, headers=headers)\n",
    "assert response.status_code in (200, 201)\n",
    "\n",
    "print(response)\n",
    "#print(response.json()) ## Uncomment for verbose schema\n",
    "assert \"default\" in response.json().keys()\n",
    "trt_inference_specs = response.json()[\"default\"]\n",
    "print(json.dumps(trt_inference_specs, sort_keys=True, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9168b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply changes to the specs you want to modify\n",
    "print(json.dumps(trt_inference_specs, sort_keys=True, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370cca96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run action\n",
    "parent = job_map[\"model_gen_trt_engine_\" + model_name]\n",
    "action = \"inference\"\n",
    "data = json.dumps({\"parent_job_id\":parent,\"action\":action,\"specs\":trt_inference_specs})\n",
    "\n",
    "endpoint = f\"{base_url}/experiments/{experiment_id}/jobs\"\n",
    "\n",
    "response = requests.post(endpoint, data=data, headers=headers)\n",
    "assert response.status_code in (200, 201)\n",
    "assert response.json()\n",
    "\n",
    "print(response)\n",
    "print(response.json())\n",
    "\n",
    "job_map[\"inference_trt_\" + model_name] = response.json()\n",
    "print(job_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e309f06",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Monitor job status by repeatedly running this cell\n",
    "job_id = job_map[\"inference_trt_\" + model_name]\n",
    "endpoint = f\"{base_url}/experiments/{experiment_id}/jobs/{job_id}\"\n",
    "\n",
    "while True:\n",
    "    clear_output(wait=True)\n",
    "    response = requests.get(endpoint, headers=headers)\n",
    "    assert response.status_code in (200, 201)\n",
    "    print(response)\n",
    "    print(response.json())\n",
    "    assert \"status\" in response.json().keys() and response.json().get(\"status\") != \"Error\"\n",
    "    if response.json().get(\"status\") in [\"Done\",\"Error\", \"Canceled\"] or response.status_code not in (200,201):\n",
    "        break\n",
    "    time.sleep(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44f030f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Download job contents once the above job shows \"Done\" status\n",
    "if download_jobs:\n",
    "    job_id = job_map[\"inference_trt_\" + model_name]\n",
    "    endpoint = f\"{base_url}/experiments/{experiment_id}/jobs/{job_id}\"\n",
    "    response = requests.get(endpoint, headers=headers)\n",
    "    assert response.status_code in (200, 201)\n",
    "    expected_file_size = response.json().get(\"job_tar_stats\", {}).get(\"file_size\")\n",
    "    print(\"expected_file_size: \", expected_file_size)\n",
    "\n",
    "    !python3 -m pip install tqdm\n",
    "    from tqdm import tqdm\n",
    "\n",
    "    endpoint = f'{base_url}/experiments/{experiment_id}/jobs/{job_id}:download'\n",
    "    temptar = f'{job_id}.tar.gz'\n",
    "\n",
    "    with tqdm(total=expected_file_size, unit='B', unit_scale=True) as progress_bar:\n",
    "        while True:\n",
    "            # Check if the file already exists\n",
    "            headers_download_job = dict(headers)\n",
    "            if os.path.exists(temptar):\n",
    "                # Get the current file size\n",
    "                file_size = os.path.getsize(temptar)\n",
    "                print(f\"File size of dowloaded content until now is {file_size}\")\n",
    "\n",
    "                # If the file size matches the expected size, break out of the loop\n",
    "                if file_size >= (expected_file_size-1):\n",
    "                    print(\"Download completed successfully.\")\n",
    "                    print(\"Untarring\")\n",
    "                    # Untar to destination\n",
    "                    tar_command = f'tar -xf {temptar} -C {workdir}/'\n",
    "                    os.system(tar_command)\n",
    "                    os.remove(temptar)\n",
    "                    print(f\"Results at {workdir}/{job_id}\")\n",
    "                    inference_out_path = f\"{workdir}/{job_id}\"\n",
    "                    break\n",
    "\n",
    "                # Set the headers to resume the download from where it left off\n",
    "                headers_download_job['Range'] = f'bytes={file_size}-'\n",
    "            # Open the file for writing in binary mode\n",
    "            with open(temptar, 'ab') as f:\n",
    "                try:\n",
    "                    response = requests.get(endpoint, headers=headers_download_job, stream=True)\n",
    "                    print(response)\n",
    "                    # Check if the request was successful\n",
    "                    if response.status_code in [200, 206]:\n",
    "                        # Iterate over the content in chunks\n",
    "                        for chunk in response.iter_content(chunk_size=1024):\n",
    "                            if chunk:\n",
    "                                # Write the chunk to the file\n",
    "                                f.write(chunk)\n",
    "                                # Flush and sync the file to disk\n",
    "                                f.flush()\n",
    "                                os.fsync(f.fileno())\n",
    "                            progress_bar.update(len(chunk))\n",
    "                    else:\n",
    "                        print(f\"Failed to download file. Status code: {response.status_code}\")\n",
    "                except requests.exceptions.RequestException as e:\n",
    "                    print(\"Connection interrupted during download, resuming download from breaking point\")\n",
    "                    time.sleep(5)  # Sleep for a while before retrying the request\n",
    "                    continue  # Continue the loop to retry the request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f193a832",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Print Classification results\n",
    "if download_jobs:\n",
    "    if model_name in (\"classification_tf1\", \"multitask_classification\"):\n",
    "        !cat {inference_out_path}/result.csv\n",
    "    elif \"classification_\" in model_name:\n",
    "        !cat {inference_out_path}/inference/result.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95de2992",
   "metadata": {},
   "source": [
    "### Delete model <a class=\"anchor\" id=\"head-21\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1e5701",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = f\"{base_url}/experiments/{experiment_id}\"\n",
    "\n",
    "response = requests.delete(endpoint,headers=headers)\n",
    "assert response.status_code in (200, 201)\n",
    "\n",
    "print(response)\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6770956",
   "metadata": {},
   "source": [
    "### Delete dataset <a class=\"anchor\" id=\"head-21\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac31faf",
   "metadata": {},
   "source": [
    "#### Delete train dataset <a class=\"anchor\" id=\"head-21\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02fe0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = f\"{base_url}/datasets/{train_dataset_id}\"\n",
    "\n",
    "response = requests.delete(endpoint,headers=headers)\n",
    "assert response.status_code in (200, 201)\n",
    "\n",
    "print(response)\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7501b23",
   "metadata": {},
   "source": [
    "#### Delete val dataset <a class=\"anchor\" id=\"head-21\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa99b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = f\"{base_url}/datasets/{eval_dataset_id}\"\n",
    "\n",
    "response = requests.delete(endpoint,headers=headers)\n",
    "assert response.status_code in (200, 201)\n",
    "\n",
    "print(response)\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c34ac707",
   "metadata": {},
   "source": [
    "#### Delete test dataset <a class=\"anchor\" id=\"head-21\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be2e059",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = f\"{base_url}/datasets/{test_dataset_id}\"\n",
    "\n",
    "response = requests.delete(endpoint,headers=headers)\n",
    "assert response.status_code in (200, 201)\n",
    "\n",
    "print(response)\n",
    "print(response.json())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
